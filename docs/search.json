[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Victoria Sass",
    "section": "",
    "text": "My research interests broadly focus on health disparities, the social construction of disease, and the relationship between physical and mental health outcomes. My dissertation seeks to better understand the long-term mental and physical health effects of a weight-centric health paradigm through the mechanism of dieting for weight-loss and/or weight-maintenance.\n \n  \n   \n  \n    \n     Email\n  \n  \n    \n     Google Scholar\n  \n  \n    \n     GitHub\n  \n  \n    \n     CV"
  },
  {
    "objectID": "workshops/intro_r_rstudio.html#title-slide",
    "href": "workshops/intro_r_rstudio.html#title-slide",
    "title": "",
    "section": "",
    "text": "Introduction to R & RStudio\nIntroduction to R & RStudio\nCSSCR Workshop\nCSSCR Workshop\nVictoria Sass"
  },
  {
    "objectID": "workshops/text_analysis.html#title-slide",
    "href": "workshops/text_analysis.html#title-slide",
    "title": "",
    "section": "",
    "text": "Introduction to  Text Analysis in R\nCSSCR Workshop\n30 January 2025\nVictoria Sass"
  },
  {
    "objectID": "workshops/text_analysis.html#roadmap",
    "href": "workshops/text_analysis.html#roadmap",
    "title": "",
    "section": "Roadmap",
    "text": "Roadmap\n\n\nText as Tidy Data\nSentiment Analysis\ntf-idf\nWord Embeddings\nTopic Modeling\nFurther Learning & Resources"
  },
  {
    "objectID": "workshops/text_analysis.html#section",
    "href": "workshops/text_analysis.html#section",
    "title": "",
    "section": "",
    "text": "Go to book site"
  },
  {
    "objectID": "workshops/text_analysis.html#section-1",
    "href": "workshops/text_analysis.html#section-1",
    "title": "",
    "section": "",
    "text": "Go to book site"
  },
  {
    "objectID": "workshops/text_analysis.html#the-tidyverse",
    "href": "workshops/text_analysis.html#the-tidyverse",
    "title": "",
    "section": "The tidyverse",
    "text": "The tidyverse\n\nThe tidyverse refers to two things:\n\n\na specific package in R that loads several core packages within the tidyverse.\n\na specific design philosophy, grammar, and focus on “tidy” data structures developed by Hadley Wickham 1 and the team at Posit (formerly known as R Studio).\n\n\nYou can read the official manifesto here."
  },
  {
    "objectID": "workshops/text_analysis.html#the-tidyverse-package",
    "href": "workshops/text_analysis.html#the-tidyverse-package",
    "title": "",
    "section": "The tidyverse package",
    "text": "The tidyverse package\n\n\nThe core packages within the tidyverse include:\n\n\nggplot2 (visualizations)\ndplyr (data manipulation)\ntidyr (data reshaping)\nreadr (data import/export)\npurrr (iteration)\ntibble (modern dataframe)\nstringr (text data)\nforcats (factors)"
  },
  {
    "objectID": "workshops/text_analysis.html#the-tidyverse-philosophy",
    "href": "workshops/text_analysis.html#the-tidyverse-philosophy",
    "title": "",
    "section": "The tidyverse philosophy",
    "text": "The tidyverse philosophy\n\n\nThe principles underlying the tidyverse are:\n\n\nReuse existing data structures.\nCompose simple functions with the pipe.\nEmbrace functional programming.\nDesign for humans."
  },
  {
    "objectID": "workshops/text_analysis.html#what-is-tidy-data",
    "href": "workshops/text_analysis.html#what-is-tidy-data",
    "title": "",
    "section": "What is Tidy Data?",
    "text": "What is Tidy Data?\n 1\nIllustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst"
  },
  {
    "objectID": "workshops/text_analysis.html#why-do-we-want-tidy-data",
    "href": "workshops/text_analysis.html#why-do-we-want-tidy-data",
    "title": "",
    "section": "Why do we Want Tidy Data?",
    "text": "Why do we Want Tidy Data?\n\n\nEasier to understand many rows than many columns1\nRequired for plotting in ggplot22\nRequired for many types of statistical procedures (e.g. hierarchical or mixed effects models)\nFewer issues with missing values and “imbalanced” repeated measures data\nHaving a consistent method for storing data means it’s easier to learn the tools to work with it since there’s an underlying uniformity.\n\n\n\n\nMost real-world data is not tidy because data are often organized for goals other than analysis (i.e. data entry; not intentionally created as data…) and most people aren’t familiar with the principles of tidy data.\n\n\nPlacing variables in columns also leverages R’s vectorized nature, i.e. most built-in R functions work with values of vectors.In fact, all tidyverse functions are designed to work with tidy data."
  },
  {
    "objectID": "workshops/text_analysis.html#why-do-we-want-tidy-data-1",
    "href": "workshops/text_analysis.html#why-do-we-want-tidy-data-1",
    "title": "",
    "section": "Why do we Want Tidy Data?",
    "text": "Why do we Want Tidy Data?\n 1\nIllustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst"
  },
  {
    "objectID": "workshops/text_analysis.html#data-from-bobsburgersr",
    "href": "workshops/text_analysis.html#data-from-bobsburgersr",
    "title": "",
    "section": "Data from bobsburgersR",
    "text": "Data from bobsburgersR\n\n\n1install.packages(\"pak\")\npak(\"poncest/bobsburgersR\")\n\n\n1\n\nRun these two lines of code in your console.\n\n\n\n\n\n\nThe pak package is an alternative to using install.packages() and, for our purposes here, allows us to download our data directly from Github.\n\n\n\nlibrary(tidyverse)\nlibrary(bobsburgersR)\nhead(transcript_data, n = 10)\n\n\n\n# A tibble: 10 × 6\n   season episode title        line raw_text                            dialogue\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;                               &lt;chr&gt;   \n 1      1       1 Human Flesh     1 &lt;NA&gt;                                &lt;NA&gt;    \n 2      1       1 Human Flesh     2 &lt;NA&gt;                                &lt;NA&gt;    \n 3      1       1 Human Flesh     3 &lt;NA&gt;                                &lt;NA&gt;    \n 4      1       1 Human Flesh     4 Listen, pep talk.                   Listen,…\n 5      1       1 Human Flesh     5 Big day today.                      Big day…\n 6      1       1 Human Flesh     6 It's our grand re-re-re-opening.    It's ou…\n 7      1       1 Human Flesh     7 It's labor day weekend, And it loo… It's la…\n 8      1       1 Human Flesh     8 So we have to-- Big day for anothe… So we h…\n 9      1       1 Human Flesh     9 Go ahead, sorry.                    Go ahea…\n10      1       1 Human Flesh    10 Go ahead, do your pep.              Go ahea…"
  },
  {
    "objectID": "workshops/text_analysis.html#first-steps",
    "href": "workshops/text_analysis.html#first-steps",
    "title": "",
    "section": "First steps",
    "text": "First steps\n\n\nlibrary(tidytext) \n2library(textclean)\n\n3clean_text &lt;- function(text) {\n  text |&gt; \n4    replace_html() |&gt;\n5    replace_non_ascii() |&gt;\n6    replace_contraction() |&gt;\n7    replace_white() |&gt;\n8    tolower() |&gt;\n9    str_remove_all(pattern = \"[:punct:]|[:symbol:]\")\n}\n\n\n2\n\nYou can optionally run check_text() on your raw text variable to receive a print-out of possible functions you may want to use to clean your raw text data.\n\n3\n\nCreating our own helper function to clean our raw text variable\n\n4\n\nRemoves any HTML tags\n\n5\n\nRemoves non_ASCII characters\n\n6\n\nExpands contractions to avoid repetition\n\n7\n\nCollapses multiple spaces\n\n8\n\nNormalizes all lower case\n\n9\n\nRemoves all punctuation/symbols"
  },
  {
    "objectID": "workshops/text_analysis.html#cleaning-the-data",
    "href": "workshops/text_analysis.html#cleaning-the-data",
    "title": "",
    "section": "Cleaning the data",
    "text": "Cleaning the data\n\nclean_bob &lt;- transcript_data |&gt; \n10  drop_na() |&gt;\n  mutate(cleaned_text = raw_text |&gt; clean_text()) \n\n\n10\n\nDrops any rows in a dataframe that have NAs in them\n\n\n\n\n\n\n\n\n\nclean_bob |&gt; \n  select(raw_text) |&gt; \n  slice_sample(n = 5) \n\n\n\n\n\n# A tibble: 5 × 1\n  raw_text                                                                      \n  &lt;chr&gt;                                                                         \n1 Yeah, come, Susmita.                                                          \n2 And the heat is off in the restaurant,                                        \n3 Whoa! Oh.                                                                     \n4 I was gonna give it to you for your birthday  and, uh, surprise you, but it's…\n5 It'll be just like at the doctor,  but we've both seen each other naked.      \n\n\n\n\n\n\nclean_bob |&gt; \n  select(cleaned_text) |&gt; \n  slice_sample(n = 5) \n\n\n\n\n\n# A tibble: 5 × 1\n  cleaned_text                                                                  \n  &lt;chr&gt;                                                                         \n1 yeah come susmita                                                             \n2 and the heat is off in the restaurant                                         \n3 whoa oh                                                                       \n4 i was gonna give it to you for your birthday and uh surprise you but it is in…\n5 it will be just like at the doctor but we have both seen each other naked"
  },
  {
    "objectID": "workshops/text_analysis.html#tokenization",
    "href": "workshops/text_analysis.html#tokenization",
    "title": "",
    "section": "Tokenization",
    "text": "Tokenization\n\n\nIn order to create a tidy dataset from lots of text, we need to define a feature that we want to use as our unit of analysis.\nThis could be many different things (i.e. a sentence, a paragraph, an n-gram, etc.) but the most common token is a word."
  },
  {
    "objectID": "workshops/text_analysis.html#tidying-the-data",
    "href": "workshops/text_analysis.html#tidying-the-data",
    "title": "",
    "section": "Tidying the data",
    "text": "Tidying the data\n\ntidy_bob &lt;- clean_bob |&gt; \n  left_join(transcript_data |&gt;\n              select(season, episode) |&gt; \n              distinct() |&gt; \n11              mutate(episode_seq = row_number())) |&gt;\n12  select(-c(dialogue, raw_text)) |&gt;\n13  unnest_tokens(output = word, input = cleaned_text)\n\ntidy_bob\n\n\n11\n\nThis entire left_join dataset simply creates an episode number that isn’t broken up by season\n\n12\n\nRemoves the dialogue and raw_text columns\n\n13\n\nThis function tokenizes our data into a tidy format\n\n\n\n\n\n\n# A tibble: 1,127,930 × 6\n   season episode title        line episode_seq word  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;       &lt;int&gt; &lt;chr&gt; \n 1      1       1 Human Flesh     4           1 listen\n 2      1       1 Human Flesh     4           1 pep   \n 3      1       1 Human Flesh     4           1 talk  \n 4      1       1 Human Flesh     5           1 big   \n 5      1       1 Human Flesh     5           1 day   \n 6      1       1 Human Flesh     5           1 today \n 7      1       1 Human Flesh     6           1 it    \n 8      1       1 Human Flesh     6           1 is    \n 9      1       1 Human Flesh     6           1 our   \n10      1       1 Human Flesh     6           1 grand \n# ℹ 1,127,920 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#stop-words",
    "href": "workshops/text_analysis.html#stop-words",
    "title": "",
    "section": "Stop Words",
    "text": "Stop Words\nAs you might imagine, most words that appear in a corpus of text are going to be fairly boring and (depending on the analysis) not very insightful. These are what’s known as stop words.\n\n\n\n\ndata(stop_words)\nstop_words\n\n\n\n# A tibble: 1,149 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# ℹ 1,139 more rows\n\n\n\n\n\n\nstop_words |&gt; count(lexicon)\n\n\n\n# A tibble: 3 × 2\n  lexicon      n\n  &lt;chr&gt;    &lt;int&gt;\n1 SMART      571\n2 onix       404\n3 snowball   174"
  },
  {
    "objectID": "workshops/text_analysis.html#removing-stop-words",
    "href": "workshops/text_analysis.html#removing-stop-words",
    "title": "",
    "section": "Removing Stop Words",
    "text": "Removing Stop Words\n\n\ntidy_bob &lt;- tidy_bob |&gt; \n  anti_join(stop_words)\n\ntidy_bob\n\n\n\n# A tibble: 355,240 × 6\n   season episode title        line episode_seq word         \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;       &lt;int&gt; &lt;chr&gt;        \n 1      1       1 Human Flesh     4           1 listen       \n 2      1       1 Human Flesh     4           1 pep          \n 3      1       1 Human Flesh     4           1 talk         \n 4      1       1 Human Flesh     5           1 day          \n 5      1       1 Human Flesh     6           1 grand        \n 6      1       1 Human Flesh     6           1 rerereopening\n 7      1       1 Human Flesh     7           1 labor        \n 8      1       1 Human Flesh     7           1 day          \n 9      1       1 Human Flesh     7           1 weekend      \n10      1       1 Human Flesh     7           1 wharf        \n# ℹ 355,230 more rows\n\n\n\n\n\nFor our purposes today we’ll be removing stop words but know that stop words are not (always) just filler. They contain (low-level) information and depending upon the goal of your research, they may help inform your subsequent analyses."
  },
  {
    "objectID": "workshops/text_analysis.html#term-frequency",
    "href": "workshops/text_analysis.html#term-frequency",
    "title": "",
    "section": "Term Frequency",
    "text": "Term Frequency\nThe most basic statistic we can derive after these preliminary steps is calculating which words are used most often in our data.\n\n\ntidy_bob |&gt; \n14  count(word, sort = TRUE)\n\n\n14\n\nThis is the same as group_by(word) |&gt; summarise(n = n()) |&gt; arrange(desc(n))\n\n\n\n\n\n\n# A tibble: 25,814 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 yeah    6444\n 2 uh      5320\n 3 gonna   4687\n 4 bob     3576\n 5 tina    3157\n 6 hey     2787\n 7 gene    2557\n 8 wait    2314\n 9 god     2235\n10 louise  2002\n# ℹ 25,804 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#term-frequency-visualization",
    "href": "workshops/text_analysis.html#term-frequency-visualization",
    "title": "",
    "section": "Term Frequency Visualization",
    "text": "Term Frequency Visualization\n\nCodePlot\n\n\n\nobservable &lt;- c(\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\",\"#9467bd\",\n15                \"#8c564b\",\"#e377c2\",\"#7f7f7f\",\"#bcbd22\",\"#17becf\")\n\ntidy_bob |&gt; \n  group_by(season) |&gt; \n  count(word, sort = TRUE) |&gt;\n16  slice_max(order_by = n, n = 7) |&gt;\n  ungroup() |&gt; \n17  mutate(word_color = fct_reorder(word, n),\n18         word = reorder_within(word, n, season)) |&gt;\n  ggplot(aes(x = n, y = word, fill = word_color)) +\n  geom_col(show.legend = FALSE) +\n  labs(y = NULL) + \n19  scale_y_reordered() +\n20  scale_fill_manual(values = observable) +\n21  facet_wrap( ~ season, scales = \"free_y\") +\n  theme_minimal(base_size = 18)\n\n\n15\n\nCreate an inclusive color palette (i.e. visually distinct colors for those with colorblindness)\n\n16\n\nOnly get the top 7 words per season (which returns the top 10 words overall for these data)\n\n17\n\nSince reorder_within is a bit of a workaround, to apply our own color scheme we need to create a version of our variable explicitly for the fill argument (see this issue for reference)\n\n18\n\nReorder the words by their count per season\n\n19\n\nThis argument pairs with reorder_within to properly label the final plots\n\n20\n\nApplies our color palette to the 10 most common words in the data\n\n21\n\nCreates small multiples of the top 7 most common words by each season"
  },
  {
    "objectID": "workshops/text_analysis.html#sentiment-analysis-and-tidy-data",
    "href": "workshops/text_analysis.html#sentiment-analysis-and-tidy-data",
    "title": "",
    "section": "Sentiment Analysis and Tidy Data",
    "text": "Sentiment Analysis and Tidy Data\n\n\nSentiment analysis (aka opinion mining) can be used to get a sense of a source’s attitudes or emotions in a section of writing\nWe can use pre-existing sentiment dictionaries, create our own, or do a combination of the two\nWords can be categorized in a number of ways:\n\nAs a simple binary of negative or positive\nAlong a numeric/likert scale from most negative to most positive\nAccording to a set of particular emotions/attitudes they represent\n\nThese measures can then be summarized to represent the overall sentiment or the sentiment within some subset of text (pages, chapters, books, authors, etc.)."
  },
  {
    "objectID": "workshops/text_analysis.html#sentiments-datasets",
    "href": "workshops/text_analysis.html#sentiments-datasets",
    "title": "",
    "section": "sentiments datasets",
    "text": "sentiments datasets\n\n1library(textdata)\n\n\n1\n\nRequired to download the AFINN and nrc datasets\n\n\n\n\n\n\nAFINNbingnrc\n\n\n\n2get_sentiments(\"afinn\")\n\n\n2\n\nScale from -5 to 5 delineating most negative to most positive\n\n\n\n\n# A tibble: 2,477 × 2\n   word       value\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows\n\n\n\n\n\n3get_sentiments(\"bing\")\n\n\n3\n\nBinary categorization of positive/negative\n\n\n\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n\n\n\n\n\n4get_sentiments(\"nrc\")\n\n\n4\n\nCategorizations of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust\n\n\n\n\n# A tibble: 13,872 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# ℹ 13,862 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#compare-lexicons",
    "href": "workshops/text_analysis.html#compare-lexicons",
    "title": "",
    "section": "Compare Lexicons",
    "text": "Compare Lexicons\n\nafinn &lt;- tidy_bob |&gt; \n5  inner_join(get_sentiments(\"afinn\")) |&gt;\n6  group_by(episode_seq) |&gt;\n7  summarise(sentiment = sum(value)) |&gt;\n8  mutate(method = \"AFINN\")\n\nbing_and_nrc &lt;- bind_rows(\n  \n  tidy_bob |&gt; \n    inner_join(get_sentiments(\"bing\"))  |&gt; \n    mutate(method = \"Bing et al.\"),\n  \n  tidy_bob |&gt; \n    inner_join(get_sentiments(\"nrc\") |&gt;  \n9                 filter(sentiment %in% c(\"positive\", \"negative\")),\n10               relationship = \"many-to-many\"\n    )  |&gt; \n    mutate(method = \"NRC\"))  |&gt; \n  \n11  count(method, episode_seq, sentiment)  |&gt;\n12  pivot_wider(names_from = sentiment,\n13              values_from = n,\n14              values_fill = 0) |&gt;\n15  mutate(sentiment = positive - negative)\n\n\n5\n\nAttach lexicon values to tidy data\n\n6\n\nGroup the data by episode\n\n7\n\nSum the sentiment values for each token (word) for each episode\n\n8\n\nAdd a column to differentiate the different methods\n\n9\n\nKeeping only the rows with “positive” or “negative” sentiments\n\n10\n\nRepeated words will match many times with this sentiment so it’s a many-to-many merge\n\n11\n\nCount the number of positive and negative sentiments by sentiment lexicon and episode number\n\n12\n\nThis function takes the sentiment column and turns its values (positive, negative) into new columns\n\n13\n\nValues from n should fill the two new positive and negative sentiment columns\n\n14\n\nImplicit NAs (that are created by the new data structure) should be given a value of 0\n\n15\n\nCalculate overall sentiment by subtracting negative from positive counts"
  },
  {
    "objectID": "workshops/text_analysis.html#visualize-lexicon-differences",
    "href": "workshops/text_analysis.html#visualize-lexicon-differences",
    "title": "",
    "section": "Visualize Lexicon Differences",
    "text": "Visualize Lexicon Differences\n\nCodeHistogramCodeLoess Curve\n\n\n\nbind_rows(afinn, bing_and_nrc) |&gt; \n16  mutate(sign = ifelse(sentiment &gt; 0, \"positive\", \"negative\")) |&gt;\n17  ggplot(aes(episode_seq, sentiment, fill = method, alpha = sign)) +\n18  geom_col(show.legend = FALSE) +\n19  scale_alpha_manual(values = c(0.5, 1)) +\n20  scale_fill_manual(values = observable[1:3]) +\n21  facet_wrap(~ method, ncol = 1, scales = \"free_y\") +\n22  theme_minimal(base_size = 18)\n\n\n16\n\nCreate a variable to distinguish positive/negative explicitly\n\n17\n\nCreate a plot with episode number on the x-axis, sentiment on the y-axis, color the plot by method using fill, and create differences in transparency (alpha) by sign\n\n18\n\nUse columns to represent the data\n\n19\n\nChange alpha transparency to 50% for negative and keep at 100% (no transparency) for positive\n\n20\n\nManually change the fill colors to our pre-defined observable palette\n\n21\n\nCreate small multiples by the lexicon method and allow the y-axis to vary based on their respective data ranges\n\n22\n\nApply ggplot2’s minimal theme and increase the base font size\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbind_rows(afinn, bing_and_nrc) |&gt; \n23  ggplot(aes(episode_seq, sentiment, color = method)) +\n24  geom_smooth(se = FALSE, span = 0.4) +\n  scale_color_manual(values = observable[1:3]) + \n  theme_minimal(base_size = 18) +\n  theme(legend.position = \"bottom\") \n\n\n23\n\nUse color here instead of fill since we’re mapping a line instead of a column\n\n24\n\nCreates a moving average of sentiments (averaging window defined by span; se controls whether to show confidence intervals)"
  },
  {
    "objectID": "workshops/text_analysis.html#putting-it-all-together",
    "href": "workshops/text_analysis.html#putting-it-all-together",
    "title": "",
    "section": "Putting it all together",
    "text": "Putting it all together\nWhat if we want to see the top words broken down by sentiment and lexicon?\n\n\nafinn &lt;- tidy_bob |&gt; \n  inner_join(get_sentiments(\"afinn\")) |&gt; \n26  mutate(sentiment = if_else(value &gt; 0, \"positive\", \"negative\")) |&gt;\n  count(word, sentiment, sort = TRUE) |&gt;\n  mutate(method = \"AFINN\")\n\nbing &lt;- tidy_bob |&gt; \n  inner_join(get_sentiments(\"bing\")) |&gt; \n  count(word, sentiment, sort = TRUE) |&gt; \n  mutate(method = \"Bing et al.\")\n\n\nnrc &lt;- tidy_bob |&gt; \n  inner_join(get_sentiments(\"nrc\") |&gt; \n               filter(sentiment %in% c(\"positive\", \"negative\"))) |&gt; \n  count(word, sentiment, sort = TRUE) |&gt; \n  mutate(method = \"NRC\")\n\n\n26\n\nRecoding this sentiment to negative if value &lt; 0 and positive if value &gt; 0"
  },
  {
    "objectID": "workshops/text_analysis.html#top-words-by-sentiment-lexicon",
    "href": "workshops/text_analysis.html#top-words-by-sentiment-lexicon",
    "title": "",
    "section": "Top Words by Sentiment & Lexicon",
    "text": "Top Words by Sentiment & Lexicon\n\nlibrary(wordcloud) \n27library(reshape2)\n\n\n27\n\nNeeded to restructure dataset for wordclouds\n\n\n\n\n\nCodeAFINNCodebingCodenrc\n\n\n\nafinn |&gt; \n  arrange(desc(n)) |&gt;\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) |&gt; \n  comparison.cloud(colors = observable[c(4, 1)], \n                   title.size = 2, \n                   scale=c(4, 1),\n                   max.words = 100)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbing |&gt; \n  arrange(desc(n)) |&gt;\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) |&gt; \n  comparison.cloud(colors = observable[c(4, 1)], \n                   scale=c(4, 1),\n                   title.size = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnrc |&gt; \n  arrange(desc(n)) |&gt;\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) |&gt; \n  comparison.cloud(colors = observable[c(4, 1)], \n                   title.size = 2)"
  },
  {
    "objectID": "workshops/text_analysis.html#word-frequency",
    "href": "workshops/text_analysis.html#word-frequency",
    "title": "",
    "section": "Word Frequency",
    "text": "Word Frequency\n\nMost documents will have a lot of some words and not very many of much fewer words\n\n\n\n\n\n1season_words &lt;- clean_bob |&gt;\n  select(season, cleaned_text) |&gt;\n  unnest_tokens(word, cleaned_text) |&gt;\n  count(season, word, sort = TRUE)\n\nseason_words\n\n\n1\n\nGets the count of each word by season\n\n\n\n\n\n\n# A tibble: 83,633 × 3\n   season word      n\n    &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;\n 1     13 i      4024\n 2     12 i      3886\n 3     11 i      3832\n 4     10 i      3816\n 5      8 i      3573\n 6      7 i      3527\n 7      4 i      3476\n 8      9 i      3407\n 9      5 i      3406\n10      3 i      3385\n# ℹ 83,623 more rows\n\n\n\n\n2total_words &lt;- season_words |&gt;\n  group_by(season) |&gt;\n  summarize(total = sum(n))\n\ntotal_words\n\n\n2\n\nCreates a count of total words by season\n\n\n\n\n\n\n# A tibble: 14 × 2\n   season total\n    &lt;dbl&gt; &lt;int&gt;\n 1      1 44304\n 2      2 31904\n 3      3 84253\n 4      4 86537\n 5      5 85996\n 6      6 78691\n 7      7 90347\n 8      8 90281\n 9      9 92870\n10     10 94967\n11     11 95939\n12     12 97485\n13     13 97133\n14     14 57223"
  },
  {
    "objectID": "workshops/text_analysis.html#word-frequency-by-season",
    "href": "workshops/text_analysis.html#word-frequency-by-season",
    "title": "",
    "section": "Word Frequency by Season",
    "text": "Word Frequency by Season\n\nCodeVisualization\n\n\n\nlibrary(colorspace)\nlibrary(ggridges) \n3season_words &lt;- left_join(season_words, total_words)\n\n4observable_light &lt;- lighten(observable, 0.5)\n\nggplot(season_words, aes(x = n/total, y = season, fill = as_factor(season))) +\n5  geom_density_ridges(show.legend = FALSE) +\n  xlim(NA, 0.0009) +\n6  scale_fill_manual(values = c(observable, observable_light)[order(rep(seq_along(observable), 2))][1:14]) +\n  theme_minimal(base_size = 18)\n\n\n3\n\nCombines both datasets by season\n\n4\n\nCreate version of observable color palette that is 50% lighter\n\n5\n\nA fun geom from the ggridges package that allows you to visualize many density curves at once\n\n6\n\nSwitching between observable and observable_light to utilize a larger (14) color palette"
  },
  {
    "objectID": "workshops/text_analysis.html#tf-versus-tf-idf",
    "href": "workshops/text_analysis.html#tf-versus-tf-idf",
    "title": "",
    "section": "tf versus tf-idf",
    "text": "tf versus tf-idf\n\n\n\n\nSo far we’ve been looking at term frequency (tf)\nAnother approach is to look at a term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents\nThis can be combined with term frequency to calculate a term’s tf-idf (the two quantities multiplied together), which is the frequency of a term adjusted for how rarely it is used\n\n\n\n\n\nlibrary(topicmodels)\n\nseason_tf_idf &lt;- season_words |&gt; \n  bind_tf_idf(word, season, n)\n\nseason_tf_idf\n\n\n\n# A tibble: 83,633 × 7\n   season word      n total     tf   idf tf_idf\n    &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     13 i      4024 97133 0.0414     0      0\n 2     12 i      3886 97485 0.0399     0      0\n 3     11 i      3832 95939 0.0399     0      0\n 4     10 i      3816 94967 0.0402     0      0\n 5      8 i      3573 90281 0.0396     0      0\n 6      7 i      3527 90347 0.0390     0      0\n 7      4 i      3476 86537 0.0402     0      0\n 8      9 i      3407 92870 0.0367     0      0\n 9      5 i      3406 85996 0.0396     0      0\n10      3 i      3385 84253 0.0402     0      0\n# ℹ 83,623 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#highest-tf-idf-by-season",
    "href": "workshops/text_analysis.html#highest-tf-idf-by-season",
    "title": "",
    "section": "Highest tf-idf by Season",
    "text": "Highest tf-idf by Season\n\nRanked DataCodeBar Chart\n\n\n\nseason_tf_idf |&gt; \n  select(-total) |&gt; \n  arrange(desc(tf_idf))\n\n# A tibble: 83,633 × 6\n   season word            n       tf   idf  tf_idf\n    &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1      1 ok             66 0.00149   1.54 0.00229\n 2      2 beefsquatch    23 0.000721  2.64 0.00190\n 3     14 cricket        37 0.000647  2.64 0.00171\n 4      2 rodney         16 0.000502  2.64 0.00132\n 5      9 clem           45 0.000485  2.64 0.00128\n 6     10 wharfy         43 0.000453  2.64 0.00119\n 7     12 purrbo         42 0.000431  2.64 0.00114\n 8      8 bleaken        38 0.000421  2.64 0.00111\n 9      1 torpedo        31 0.000700  1.54 0.00108\n10      2 hanky          13 0.000407  2.64 0.00108\n# ℹ 83,623 more rows\n\n\n\n\n\nseason_tf_idf |&gt; \n  group_by(season) |&gt; \n  slice_max(tf_idf, n = 10) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = as_factor(season))) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_manual(values = c(observable, observable_light)[order(rep(seq_along(observable), 2))][1:14]) +\n  facet_wrap(~ season, ncol = 5, scales = \"free\") +\n  labs(x = \"tf-idf\", y = NULL) + \n  theme_minimal(base_size = 18)"
  },
  {
    "objectID": "workshops/text_analysis.html#two-approaches",
    "href": "workshops/text_analysis.html#two-approaches",
    "title": "",
    "section": "Two Approaches",
    "text": "Two Approaches\n\n\n\n\n\n\n\nCBOW (Continuous Bag of Words) takes the surrounding words to predict the target word. In contrast, the Skip-gram approach predicts context words based on the given target word."
  },
  {
    "objectID": "workshops/text_analysis.html#restructuring-our-data",
    "href": "workshops/text_analysis.html#restructuring-our-data",
    "title": "",
    "section": "Restructuring our Data",
    "text": "Restructuring our Data\n\n\n\nnested_eps &lt;- tidy_bob |&gt; \n1  add_count(word) |&gt;\n2  filter(n &gt;= 50) |&gt;\n  select(episode_seq, word) |&gt;\n3  nest(words = c(word))\n\nnested_eps \n\n\n1\n\nAdds the count, n, for each word; equivalent to group_by(word) |&gt; mutate(n = n())\n\n2\n\nKeep only words that occur at least 50 times\n\n3\n\nnest creates a list-column; in this case, a dataframe of all words that occurred by each episode\n\n\n\n\n\n\n# A tibble: 272 × 2\n   episode_seq words             \n         &lt;int&gt; &lt;list&gt;            \n 1           1 &lt;tibble [703 × 1]&gt;\n 2           2 &lt;tibble [768 × 1]&gt;\n 3           3 &lt;tibble [680 × 1]&gt;\n 4           4 &lt;tibble [778 × 1]&gt;\n 5           5 &lt;tibble [664 × 1]&gt;\n 6           6 &lt;tibble [792 × 1]&gt;\n 7           7 &lt;tibble [694 × 1]&gt;\n 8           8 &lt;tibble [714 × 1]&gt;\n 9           9 &lt;tibble [642 × 1]&gt;\n10          10 &lt;tibble [843 × 1]&gt;\n# ℹ 262 more rows\n\n\n\n\nWhat do these nested tibbles look like? Let’s look at words from episode 1:\n\n\n\nnested_eps[1, 2][[1]]\n\n\n\n[[1]]\n# A tibble: 703 × 1\n   word   \n   &lt;chr&gt;  \n 1 listen \n 2 talk   \n 3 day    \n 4 day    \n 5 weekend\n 6 wharf  \n 7 day    \n 8 reason \n 9 linda  \n10 middle \n# ℹ 693 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#creating-a-window-function",
    "href": "workshops/text_analysis.html#creating-a-window-function",
    "title": "",
    "section": "Creating a Window Function",
    "text": "Creating a Window Function\n\nlibrary(slider)\n\n4slide_windows &lt;- function(tbl, window_size) {\n  \n5  skipgrams &lt;- slider::slide(\n6    tbl,\n7    ~.x,\n8    .after = window_size - 1,\n9    .step = 1,\n10    .complete = TRUE\n11  )\n  \n12  safe_mutate &lt;- safely(mutate)\n  \n13  out &lt;- map2(skipgrams,\n              1:length(skipgrams),\n14              ~ safe_mutate(.x, window_id = .y))\n\n  out |&gt;\n15    list_transpose() |&gt;\n16    pluck(\"result\") |&gt;\n17    compact() |&gt;\n18    bind_rows()\n}\n\n\n4\n\nThis function identifies skip-gram windows in order to calculate the skip-gram probabilities\n\n5\n\nRead more about the slide function here\n\n6\n\nTakes in a tibble (of words, in this case)\n\n7\n\nApplies a function to the window; here we’re simply returning the words as-is\n\n8\n\nDefines how many elements after the current one should be included in the window\n\n9\n\nHow many elements to shift by when computing the window\n\n10\n\nIf TRUE, it only evaluates complete windows (i.e. entire window size is available from the current element)\n\n11\n\nskipgrams is a list-column containing a skipgram window for each word, for each episode\n\n12\n\nCreates a version of mutate that won’t break the function, rather it’ll capture results and errors in a list object for each skipgram\n\n13\n\nmap2 mutates over each of these skipgrams to append a window_id column (which is the index of the skipgram for that epidose) so we can identify them when we unpack this extremely nested words list later\n\n14\n\nThe function that map2 is iterating through our skipgrams and their index numbers with is safe_mutate, which will create a list of 2 for each skipgram it iterates through, putting successfully mutated skipgram results into result and unsuccessfully mutated skipgram results into error\n\n15\n\nTurns each episode’s words list from a list of skipgrams (each with a list of result and error per our safe_mutate function) into its transposed version, in this case two lists (result and error), each composed of the full list of skipgrams for that episode\n\n16\n\npluck pulls out the result list for each episode’s words column so now each episode’s list is composed of a list of tibbles for each skipgram widow with their corresponding window_id\n\n17\n\nRemoves empty skipgrams (i.e. where window was incomplete)\n\n18\n\nCombines all skipgram words and window_ids into one complete tibble in words column corresponding to each episode"
  },
  {
    "objectID": "workshops/text_analysis.html#lets-have-a-look-around",
    "href": "workshops/text_analysis.html#lets-have-a-look-around",
    "title": "",
    "section": "Let’s Have a Look Around 👀",
    "text": "Let’s Have a Look Around 👀\n\nlibrary(widyr)\nlibrary(furrr)\n\n19plan(multisession)\n\ntidy_pmi &lt;- nested_eps |&gt; \n20  mutate(words = future_map(words, \\(x) slide_windows(x, 4L))) |&gt;\n21  unnest(words) |&gt;\n22  unite(window_id, episode_seq, window_id) |&gt;\n23  pairwise_pmi(word, window_id)\n\n\n19\n\nAllows this process to be run computationally in parallel (huge time-saver 🙏) since each episode can be run independently of the others\n\n20\n\nTakes the words column and applies our slide_windows() function to each episode’s list with a skip-gram window of 4 words\n\n21\n\nUnnests the words column so it is no longer an embedded list per episode but instead we have a tibble with columns for episode_seq, word, and window_id. Instead of 272 rows (1 row for each episode) we now have 925,280 rows (1 row for each word in a skip-gram window for all 272 episodes)\n\n22\n\nOverwrites the window_id column to be a combination of episode_seq and window_id, separated by _\n\n23\n\nUses each instance of a word and its associated window_id to calculate the logarithm of probability of finding two words together, normalized by the probability of finding each of the words alone."
  },
  {
    "objectID": "workshops/text_analysis.html#section-6",
    "href": "workshops/text_analysis.html#section-6",
    "title": "",
    "section": "",
    "text": "tidy_pmi\n\n\n\n# A tibble: 420,500 × 3\n   item1   item2      pmi\n   &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n 1 talk    listen  0.773 \n 2 day     listen -0.224 \n 3 weekend listen  0.0570\n 4 wharf   listen -0.490 \n 5 reason  listen  0.801 \n 6 linda   listen  0.110 \n 7 middle  listen -1.46  \n 8 ahead   listen -1.10  \n 9 sell    listen  0.676 \n10 burgers listen -0.322 \n# ℹ 420,490 more rows\n\n\n\n\nOur (slide_windows) function enables us to define a fixed-size moving window around each word. Within those windows we are able to see if two given words are seen together and calculate probabilities based on those occurrences, or lack of occurrences.\ntl;dr, tidy_pmi quantifies how often we find each word near each other word."
  },
  {
    "objectID": "workshops/text_analysis.html#singular-value-decomposition-svd",
    "href": "workshops/text_analysis.html#singular-value-decomposition-svd",
    "title": "",
    "section": "Singular Value Decomposition (SVD)",
    "text": "Singular Value Decomposition (SVD)\n\n\nWe can next determine the word vectors from the PMI values using singular value decomposition (SVD)\n\nThis is a data reduction technique akin to PCA (Principle Component Analysis), i.e. it works by taking our data and decomposing it onto special orthogonal axes.\n\nThe first axis is chosen to capture as much of the variance as possible\nKeeping that first axis fixed, the remaining orthogonal axes are rotated to maximize the variance in the second axis\nThis is then repeated for all the remaining axes"
  },
  {
    "objectID": "workshops/text_analysis.html#section-7",
    "href": "workshops/text_analysis.html#section-7",
    "title": "",
    "section": "",
    "text": "tidy_word_vectors &lt;- tidy_pmi  |&gt; \n24  widely_svd(\n25    item1, item2, pmi,\n26    nv = 100,\n27    maxit = 1000\n  )\n\ntidy_word_vectors\n\n\n24\n\nTakes our tidy_pmi table, turns it into a wide matrix, and performs dimensionality reduction on it, and returns it in tidy format\n\n25\n\nitem1 serves as the item we want to perform dimensionality reduction on, item2 is the feature that links items to one another, and pmi is the value we’re reducing\n\n26\n\nNumber of principle components to estimate\n\n27\n\nOptional argument specifying the maximum number of iterations\n\n\n\n\n\n\n# A tibble: 116,600 × 3\n   item1   dimension    value\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 talk            1  0.0273 \n 2 day             1  0.0487 \n 3 weekend         1 -0.0351 \n 4 wharf           1 -0.0366 \n 5 reason          1 -0.0341 \n 6 linda           1  0.0604 \n 7 middle          1 -0.0319 \n 8 ahead           1 -0.0267 \n 9 sell            1 -0.0293 \n10 burgers         1 -0.00294\n# ℹ 116,590 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#section-8",
    "href": "workshops/text_analysis.html#section-8",
    "title": "",
    "section": "",
    "text": "Won’t You Be My Neighbor?"
  },
  {
    "objectID": "workshops/text_analysis.html#section-9",
    "href": "workshops/text_analysis.html#section-9",
    "title": "",
    "section": "",
    "text": "Won’t You Be My Neighbor?"
  },
  {
    "objectID": "workshops/text_analysis.html#dimensions-explaining-most-variation",
    "href": "workshops/text_analysis.html#dimensions-explaining-most-variation",
    "title": "",
    "section": "Dimensions Explaining Most Variation",
    "text": "Dimensions Explaining Most Variation\n\nCodeBar Chart\n\n\n\ntidy_word_vectors |&gt;\n  filter(dimension &lt;= 9) |&gt;\n  mutate(sign = if_else(value &gt; 0, \"positive\", \"negative\")) |&gt; \n  group_by(dimension) |&gt;\n  top_n(10, abs(value)) |&gt;\n  ungroup() |&gt;\n  mutate(item1 = reorder_within(item1, value, dimension)) |&gt;\n  ggplot(aes(item1, value, fill = as_factor(dimension), alpha = sign)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  scale_fill_manual(values = observable[1:9]) +\n  scale_alpha_manual(values = c(0.5, 1)) +\n  coord_flip() +\n  labs(\n    x = NULL,\n    y = \"Value\",\n    title = \"First 9 principal components for text of Bob's Burgers scripts\",\n    subtitle = \"Top words contributing to the components that explain the most variation\") + \n  theme_minimal(base_size = 18)"
  },
  {
    "objectID": "workshops/text_analysis.html#pre-trained-embeddings",
    "href": "workshops/text_analysis.html#pre-trained-embeddings",
    "title": "",
    "section": "Pre-trained Embeddings",
    "text": "Pre-trained Embeddings\n\n\nWord-embeddings work well when you have a large amount of data and our Bob dataset is a bit small\n\nIn that case, you may want to explore using pre-trained word-embeddings\n\nWe’re going to use the wordsalad package which provides some convenient functions to access some of the more popular pre-trained embeddings\n\n\n\n\nlibrary(wordsalad)\n\nglove_word_vec &lt;- \n29  glove(text = clean_bob$cleaned_text,\n        dim = 100, \n        window = 4, \n        min_count = 50, \n        stopwords = stop_words$word, \n        composition = \"tibble\")\n\nglove_word_vec\n\n\n29\n\nGlobal Vectors for Word Representation, aka GloVe\n\n\n\n\n\n\n\n# A tibble: 1,166 × 101\n   tokens         V1      V2      V3       V4      V5      V6      V7       V8\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 aa         0.0849 -0.599   0.275  -0.233    0.0862 -0.104  -0.0301 -0.169  \n 2 admit      0.718  -0.213  -0.211  -0.0504   0.508  -0.104  -0.0441  0.304  \n 3 breaking  -0.131   0.569  -0.236  -0.00908  0.447  -0.302  -0.307   0.212  \n 4 ceiling   -0.353  -0.346  -0.356  -0.418    0.0683 -0.0663  0.176  -0.183  \n 5 character -0.158   0.423   0.889  -0.247    0.536   0.0749  0.138   0.902  \n 6 cleaning   0.0753 -0.0813 -0.0965 -0.205   -0.260   0.207   0.933  -0.117  \n 7 closet     0.0156  0.157   0.391  -0.566    0.188  -0.189   0.506   0.354  \n 8 college   -0.301   0.312   0.138   0.219   -0.390   0.218  -0.428  -0.227  \n 9 corn      -0.251  -0.313  -0.508  -0.470   -0.393   0.269  -0.130   0.0282 \n10 destroy    0.314   0.0674  0.642   0.161   -0.434  -0.0652  0.770   0.00947\n# ℹ 1,156 more rows\n# ℹ 92 more variables: V9 &lt;dbl&gt;, V10 &lt;dbl&gt;, V11 &lt;dbl&gt;, V12 &lt;dbl&gt;, V13 &lt;dbl&gt;,\n#   V14 &lt;dbl&gt;, V15 &lt;dbl&gt;, V16 &lt;dbl&gt;, V17 &lt;dbl&gt;, V18 &lt;dbl&gt;, V19 &lt;dbl&gt;,\n#   V20 &lt;dbl&gt;, V21 &lt;dbl&gt;, V22 &lt;dbl&gt;, V23 &lt;dbl&gt;, V24 &lt;dbl&gt;, V25 &lt;dbl&gt;,\n#   V26 &lt;dbl&gt;, V27 &lt;dbl&gt;, V28 &lt;dbl&gt;, V29 &lt;dbl&gt;, V30 &lt;dbl&gt;, V31 &lt;dbl&gt;,\n#   V32 &lt;dbl&gt;, V33 &lt;dbl&gt;, V34 &lt;dbl&gt;, V35 &lt;dbl&gt;, V36 &lt;dbl&gt;, V37 &lt;dbl&gt;,\n#   V38 &lt;dbl&gt;, V39 &lt;dbl&gt;, V40 &lt;dbl&gt;, V41 &lt;dbl&gt;, V42 &lt;dbl&gt;, V43 &lt;dbl&gt;, …"
  },
  {
    "objectID": "workshops/text_analysis.html#tidying-our-gloves",
    "href": "workshops/text_analysis.html#tidying-our-gloves",
    "title": "",
    "section": "Tidying our GloVe(s)",
    "text": "Tidying our GloVe(s)\n\ntidy_glove &lt;- \n  glove_word_vec |&gt; \n  pivot_longer(contains(\"V\"),\n               names_to = \"dimension\", \n               names_prefix = \"V\", \n               names_transform = list(dimension = as.integer)) |&gt; \n  rename(item1 = tokens)\n\ntidy_glove\n\n\n\n# A tibble: 116,600 × 3\n   item1 dimension   value\n   &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n 1 aa            1  0.0849\n 2 aa            2 -0.599 \n 3 aa            3  0.275 \n 4 aa            4 -0.233 \n 5 aa            5  0.0862\n 6 aa            6 -0.104 \n 7 aa            7 -0.0301\n 8 aa            8 -0.169 \n 9 aa            9  0.344 \n10 aa           10  0.796 \n# ℹ 116,590 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#most-variation-pre-trained",
    "href": "workshops/text_analysis.html#most-variation-pre-trained",
    "title": "",
    "section": "Most Variation (Pre-Trained)",
    "text": "Most Variation (Pre-Trained)\n\nCodeBar Chart\n\n\n\ntidy_glove |&gt; \n  filter(dimension &lt;= 9)  |&gt; \n  mutate(sign = if_else(value &gt; 0, \"positive\", \"negative\")) |&gt; \n  group_by(dimension) |&gt;\n  top_n(10, abs(value)) |&gt;\n  ungroup() |&gt;\n  mutate(item1 = reorder_within(item1, value, dimension)) |&gt;\n  ggplot(aes(item1, value, fill = as_factor(dimension), alpha = sign)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  scale_fill_manual(values = observable[1:9]) +\n  scale_alpha_manual(values = c(0.5, 1)) +\n  coord_flip() +\n  labs(\n    x = NULL,\n    y = \"Value\",\n    title = \"First 9 principal components for text of Bob's Burgers scripts using GloVe word embeddings\",\n    subtitle = \"Top words contributing to the components that explain the most variation\") + \n  theme_minimal(base_size = 18)"
  },
  {
    "objectID": "workshops/text_analysis.html#latent-dirichlet-allocation-lda",
    "href": "workshops/text_analysis.html#latent-dirichlet-allocation-lda",
    "title": "",
    "section": "Latent Dirichlet allocation (LDA)",
    "text": "Latent Dirichlet allocation (LDA)\n\n\nA type of Bayesian network model that makes two general assumptions\n\nEach document is a mixture of topics\nEach topic is a distribution of words\n\n\n\n\n\nLDA is a mathematical method for estimating  both of these at the same time: finding the  mixture of words that is associated with each  topic, while also determining the mixture  of topics that describes each document."
  },
  {
    "objectID": "workshops/text_analysis.html#cast-tidy-data-into-a-matrix",
    "href": "workshops/text_analysis.html#cast-tidy-data-into-a-matrix",
    "title": "",
    "section": "Cast Tidy Data into a Matrix",
    "text": "Cast Tidy Data into a Matrix\n\nlibrary(topicmodels)\n\nbob_matrix &lt;- season_words |&gt; \n  anti_join(stop_words) |&gt; \n1  cast_dtm(season, word, n)\n  \nbob_matrix\n\n\n1\n\nThis function turns tidy data (with one token per row/observation) into a Document x Term matrix (necessary data structure for topic modeling)\n\n\n\n\n\n\n&lt;&lt;DocumentTermMatrix (documents: 14, terms: 25814)&gt;&gt;\nNon-/sparse entries: 76935/284461\nSparsity           : 79%\nMaximal term length: 28\nWeighting          : term frequency (tf)\n\n\n\n\n2bob_matrix$dimnames$Docs\n\n\n2\n\nThis shows us the row names of our sparse matrix (the documents that make up our corpus)\n\n\n\n\n\n\n [1] \"11\" \"12\" \"10\" \"13\" \"9\"  \"4\"  \"7\"  \"8\"  \"3\"  \"5\"  \"6\"  \"14\" \"1\"  \"2\" \n\n\n\n\n\n3bob_matrix$dimnames$Terms |&gt; head(50)\n\n\n3\n\nHere we see the first 50 column names (tokens/words) of our sparse matrix (out of 25464 total)\n\n\n\n\n\n\n [1] \"yeah\"   \"uh\"     \"gonna\"  \"bob\"    \"tina\"   \"hey\"    \"gene\"   \"god\"   \n [9] \"wait\"   \"louise\" \"teddy\"  \"guys\"   \"kids\"   \"time\"   \"dad\"    \"linda\" \n[17] \"mom\"    \"love\"   \"um\"     \"day\"    \"huh\"    \"ii\"     \"people\" \"boo\"   \n[25] \"fine\"   \"stop\"   \"whoa\"   \"ah\"     \"fun\"    \"lot\"    \"nice\"   \"lin\"   \n[33] \"bobby\"  \"stuff\"  \"guess\"  \"ooh\"    \"cat\"    \"pretty\" \"house\"  \"school\"\n[41] \"couch\"  \"bad\"    \"ha\"     \"aah\"    \"jimmy\"  \"coming\" \"ow\"     \"cool\"  \n[49] \"feel\"   \"home\""
  },
  {
    "objectID": "workshops/text_analysis.html#runing-our-lda-model",
    "href": "workshops/text_analysis.html#runing-our-lda-model",
    "title": "",
    "section": "Runing our LDA Model",
    "text": "Runing our LDA Model\n\nbob_lda &lt;- LDA(bob_matrix, \n4               k = 5,\n5               control = list(seed = 01302025))\n\nbob_lda\n\n\n4\n\nNeed to specify the number of topics (read more about how one might select the best K value here)\n\n5\n\nSet a seed so that the output of the model is predictable\n\n\n\n\n\n\nA LDA_VEM topic model with 5 topics."
  },
  {
    "objectID": "workshops/text_analysis.html#per-topic-per-word-probabilities",
    "href": "workshops/text_analysis.html#per-topic-per-word-probabilities",
    "title": "",
    "section": "Per-Topic-Per-Word Probabilities",
    "text": "Per-Topic-Per-Word Probabilities\n\nBeta MatrixTop TermsCodeBar Chart\n\n\n\nbob_topics &lt;- tidy(bob_lda, matrix = \"beta\") \n\nbob_topics\n\n# A tibble: 129,070 × 3\n   topic term    beta\n   &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1     1 yeah  0.0180\n 2     2 yeah  0.0179\n 3     3 yeah  0.0191\n 4     4 yeah  0.0173\n 5     5 yeah  0.0181\n 6     1 uh    0.0128\n 7     2 uh    0.0141\n 8     3 uh    0.0187\n 9     4 uh    0.0158\n10     5 uh    0.0136\n# ℹ 129,060 more rows\n\n\n\n\n\nbob_top_terms &lt;- bob_topics |&gt; \n  group_by(topic) |&gt; \n  slice_max(beta, n = 10) |&gt; \n  ungroup() |&gt; \n  arrange(topic, -beta)\n\nbob_top_terms\n\n# A tibble: 50 × 3\n   topic term     beta\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1     1 yeah  0.0180 \n 2     1 uh    0.0128 \n 3     1 bob   0.0120 \n 4     1 gonna 0.0120 \n 5     1 hey   0.00958\n 6     1 tina  0.00891\n 7     1 gene  0.00774\n 8     1 god   0.00627\n 9     1 dad   0.00615\n10     1 wait  0.00607\n# ℹ 40 more rows\n\n\n\n\n\nbob_top_terms |&gt; \n  mutate(term = reorder_within(term, beta, topic)) |&gt; \n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() + \n  scale_fill_manual(values = observable[1:5]) + \n  theme_minimal(base_size = 18)"
  },
  {
    "objectID": "workshops/text_analysis.html#document-topic-probabilities",
    "href": "workshops/text_analysis.html#document-topic-probabilities",
    "title": "",
    "section": "Document-topic Probabilities",
    "text": "Document-topic Probabilities\n\nGamma MatrixCodeBar Chart\n\n\n\nbob_documents &lt;- tidy(bob_lda, matrix = \"gamma\")\nbob_documents\n\n# A tibble: 70 × 3\n   document topic       gamma\n   &lt;chr&gt;    &lt;int&gt;       &lt;dbl&gt;\n 1 11           1 0.000000666\n 2 12           1 0.000000668\n 3 10           1 0.000000668\n 4 13           1 0.000000671\n 5 9            1 0.000000660\n 6 4            1 0.000000741\n 7 7            1 1.00       \n 8 8            1 0.000000699\n 9 3            1 1.00       \n10 5            1 0.00232    \n# ℹ 60 more rows\n\n\n\n\n\nbob_documents |&gt; \n  mutate(document = fct(document, levels = seq(1:14) |&gt; as.character())) |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma, n = 10) |&gt; \n  ungroup() |&gt; \n  arrange(topic, -gamma) |&gt; \n  ggplot(aes(gamma, document, fill = factor(topic))) +\n  geom_col(show.legend = FALSE, alpha = 0.75) +\n  scale_fill_manual(values = observable[1:5]) + \n  theme_minimal(base_size = 18)"
  },
  {
    "objectID": "workshops/text_analysis.html#bobs-kinda-boring",
    "href": "workshops/text_analysis.html#bobs-kinda-boring",
    "title": "",
    "section": "Bob’s kinda boring 1…",
    "text": "Bob’s kinda boring 1…\n\nA better example:\n\nlibrary(gutenbergr)\n\nbooks &lt;- gutenberg_download(c(61, 408, 833, 14977), \n                            meta_fields = c(\"title\", \"author\"))\n\nbooks |&gt; distinct(author, title)\n\n\n\n# A tibble: 5 × 2\n  author                                       title                            \n  &lt;chr&gt;                                        &lt;chr&gt;                            \n1 Marx, Karl                                   \"The Communist Manifesto\"        \n2 Engels, Friedrich                            \"The Communist Manifesto\"        \n3 Du Bois, W. E. B. (William Edward Burghardt) \"The Souls of Black Folk\"        \n4 Veblen, Thorstein                            \"The Theory of the Leisure Class\"\n5 Wells-Barnett, Ida B.                        \"The Red Record\\nTabulated Stati…\n\n\n\nAs a dataset for topic modeling, that is. Not as the perfect comfort show!"
  },
  {
    "objectID": "workshops/text_analysis.html#book-cleaning",
    "href": "workshops/text_analysis.html#book-cleaning",
    "title": "",
    "section": "Book Cleaning",
    "text": "Book Cleaning\n\nbooks &lt;- books |&gt; \n6  distinct(text, .keep_all = TRUE) |&gt;\n7  mutate(author = if_else(author == \"Marx, Karl\",\n                          \"Marx, Karl; Engels, Friedrich\",\n                          author),\n8         title = if_else(str_detect(title, \"The Red Record\"),\n                         \"The Red Record\",\n                         title))\n\nbooks\n\n\n6\n\nRemove duplicates of The Communist Manifesto\n\n7\n\nExplicitly adding Engels to Communist Manifesto authorship\n\n8\n\nTruncating The Red Record for readability\n\n\n\n\n\n\n# A tibble: 19,636 × 4\n   gutenberg_id text                                                title author\n          &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt; &lt;chr&gt; \n 1           61 \"The Communist Manifesto\"                           The … Marx,…\n 2           61 \"\"                                                  The … Marx,…\n 3           61 \"by Karl Marx and Friedrich Engels\"                 The … Marx,…\n 4           61 \"[From the English edition of 1888, edited by Frie… The … Marx,…\n 5           61 \"Contents\"                                          The … Marx,…\n 6           61 \" I. BOURGEOIS AND PROLETARIANS\"                    The … Marx,…\n 7           61 \" II. PROLETARIANS AND COMMUNISTS\"                  The … Marx,…\n 8           61 \" III. SOCIALIST AND COMMUNIST LITERATURE\"          The … Marx,…\n 9           61 \" IV. POSITION OF THE COMMUNISTS IN RELATION TO TH… The … Marx,…\n10           61 \"A spectre is haunting Europe—the spectre of Commu… The … Marx,…\n# ℹ 19,626 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#divide-each-book-into-sections",
    "href": "workshops/text_analysis.html#divide-each-book-into-sections",
    "title": "",
    "section": "Divide Each Book into Sections",
    "text": "Divide Each Book into Sections\n\nby_section &lt;- books |&gt; \n9  filter(text != \"\") |&gt;\n10  mutate(text = str_remove_all(text, \"[:digit:]\"),\n11         word_count = str_count(text, \"\\\\S+\")) |&gt;\n  group_by(title) |&gt; \n12  mutate(cumulative_words = cumsum(word_count),\n13         row_chunk = ceiling(cumulative_words / 2000)) |&gt;\n  group_by(title, row_chunk) |&gt; \n14  summarize(section = str_c(text, collapse = \" \"), .groups = \"drop\") |&gt;\n15  unite(document, title, row_chunk, sep = \"_\", remove = FALSE) |&gt;\n  select(document, section)\n\nby_section\n\n\n9\n\nRemove row with empty strings\n\n10\n\nRemove all digits from our raw text column\n\n11\n\nCount number of words per row\n\n12\n\nCalculate cumulative word count per book\n\n13\n\nCreates a row_chunk index that groups observations of ~ 2000 words together\n\n14\n\nCombines the row_chunk indices together so each book section is about the same length\n\n15\n\nCombines title and row_chunk into a new variable called document, separated by _"
  },
  {
    "objectID": "workshops/text_analysis.html#divide-each-book-into-sections-output",
    "href": "workshops/text_analysis.html#divide-each-book-into-sections-output",
    "title": "",
    "section": "Divide Each Book into Sections",
    "text": "Divide Each Book into Sections\n\n# A tibble: 112 × 2\n   document                  section                                            \n   &lt;chr&gt;                     &lt;chr&gt;                                              \n 1 The Communist Manifesto_1 \"The Communist Manifesto by Karl Marx and Friedric…\n 2 The Communist Manifesto_2 \"powers of the nether world whom he has called up …\n 3 The Communist Manifesto_3 \"the bourgeois family-relations; modern industrial…\n 4 The Communist Manifesto_4 \"abolition of bourgeois property, the standard of …\n 5 The Communist Manifesto_5 \"and deserted with loud and irreverent laughter. O…\n 6 The Communist Manifesto_6 \"Bourgeois Socialism attains adequate expression, …\n 7 The Red Record_1          \"The Red Record: Tabulated Statistics and Alleged …\n 8 The Red Record_2          \"describe as such? Not by any means. With the Sout…\n 9 The Red Record_3          \"Va.; Nov. , Samuel Motlow, Lynchburg, Va.; Nov. ,…\n10 The Red Record_4          \"  Ford was greatly hurt and the Negro was held to…\n# ℹ 102 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#create-our-tidy-text-dataset",
    "href": "workshops/text_analysis.html#create-our-tidy-text-dataset",
    "title": "",
    "section": "Create our Tidy Text Dataset",
    "text": "Create our Tidy Text Dataset\n\nby_section_word &lt;- by_section |&gt; \n  unnest_tokens(word, section)\n\nby_section_word\n\n# A tibble: 221,458 × 2\n   document                  word     \n   &lt;chr&gt;                     &lt;chr&gt;    \n 1 The Communist Manifesto_1 the      \n 2 The Communist Manifesto_1 communist\n 3 The Communist Manifesto_1 manifesto\n 4 The Communist Manifesto_1 by       \n 5 The Communist Manifesto_1 karl     \n 6 The Communist Manifesto_1 marx     \n 7 The Communist Manifesto_1 and      \n 8 The Communist Manifesto_1 friedrich\n 9 The Communist Manifesto_1 engels   \n10 The Communist Manifesto_1 from     \n# ℹ 221,448 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#find-document-word-counts",
    "href": "workshops/text_analysis.html#find-document-word-counts",
    "title": "",
    "section": "Find Document-Word Counts",
    "text": "Find Document-Word Counts\n\nword_counts &lt;- by_section_word |&gt; \n  anti_join(stop_words) |&gt; \n  count(document, word, sort = TRUE)\n\nword_counts\n\n\n\n# A tibble: 56,260 × 3\n   document                           word         n\n   &lt;chr&gt;                              &lt;chr&gt;    &lt;int&gt;\n 1 The Theory of the Leisure Class_43 class       41\n 2 The Theory of the Leisure Class_45 class       37\n 3 The Souls of Black Folk_26         negro       34\n 4 The Theory of the Leisure Class_26 life        34\n 5 The Communist Manifesto_3          property    33\n 6 The Red Record_12                  white       33\n 7 The Theory of the Leisure Class_1  class       32\n 8 The Theory of the Leisure Class_51 class       32\n 9 The Red Record_1                   white       31\n10 The Theory of the Leisure Class_33 class       31\n# ℹ 56,250 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#section-12",
    "href": "workshops/text_analysis.html#section-12",
    "title": "",
    "section": "",
    "text": "Back into the matrix…"
  },
  {
    "objectID": "workshops/text_analysis.html#section-13",
    "href": "workshops/text_analysis.html#section-13",
    "title": "",
    "section": "",
    "text": "Back into the matrix…"
  },
  {
    "objectID": "workshops/text_analysis.html#visualize-top-terms",
    "href": "workshops/text_analysis.html#visualize-top-terms",
    "title": "",
    "section": "Visualize Top Terms",
    "text": "Visualize Top Terms\n\nBeta MatrixCodeBar Chart\n\n\n\nsections_topics &lt;- tidy(sections_lda, matrix = \"beta\")\nsections_topics |&gt; \n  arrange(-beta)\n\n# A tibble: 55,376 × 3\n   topic term        beta\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     2 class    0.0216 \n 2     2 leisure  0.0163 \n 3     1 life     0.0161 \n 4     2 life     0.0127 \n 5     3 negro    0.0110 \n 6     3 white    0.00946\n 7     4 life     0.00904\n 8     1 class    0.00884\n 9     2 economic 0.00795\n10     3 black    0.00774\n# ℹ 55,366 more rows\n\n\n\n\n\ntop_terms &lt;- sections_topics |&gt; \n  group_by(topic) |&gt; \n  slice_max(beta, n = 5) |&gt; \n  ungroup() |&gt; \n  arrange(topic, -beta)\n\ntop_terms |&gt; \n  mutate(term = reorder_within(term, beta, topic)) |&gt; \n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_manual(values = observable[1:4]) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() + \n  theme_minimal(base_size = 18)"
  },
  {
    "objectID": "workshops/text_analysis.html#per-document-classification",
    "href": "workshops/text_analysis.html#per-document-classification",
    "title": "",
    "section": "Per-document classification",
    "text": "Per-document classification\n\nGamma MatrixCodeBoxplot\n\n\n\nsections_gamma &lt;- tidy(sections_lda, matrix = \"gamma\")\nsections_gamma\n\n# A tibble: 448 × 3\n   document                           topic     gamma\n   &lt;chr&gt;                              &lt;int&gt;     &lt;dbl&gt;\n 1 The Theory of the Leisure Class_43     1 0.0000349\n 2 The Theory of the Leisure Class_45     1 0.0000349\n 3 The Souls of Black Folk_26             1 0.0000320\n 4 The Theory of the Leisure Class_26     1 1.00     \n 5 The Communist Manifesto_3              1 1.00     \n 6 The Red Record_12                      1 0.0000343\n 7 The Theory of the Leisure Class_1      1 0.0000347\n 8 The Theory of the Leisure Class_51     1 0.0000372\n 9 The Red Record_1                       1 0.0000351\n10 The Theory of the Leisure Class_33     1 0.799    \n# ℹ 438 more rows\n\n\n\n\n\nsections_gamma &lt;- sections_gamma |&gt; \n  separate(document, c(\"title\", \"section\"), sep = \"_\", convert = TRUE)\n\nsections_gamma |&gt; \n  mutate(title = reorder(title, gamma * topic)) |&gt; \n  ggplot(aes(factor(topic), gamma, fill = factor(topic))) +\n  geom_boxplot(alpha = 0.5, show.legend = FALSE) +\n  scale_fill_manual(values = observable[1:4]) +\n  facet_wrap(~ title) +\n  labs(x = \"topic\", y = expression(gamma)) + \n  theme_minimal(base_size = 18)"
  },
  {
    "objectID": "workshops/text_analysis.html#section-classifications",
    "href": "workshops/text_analysis.html#section-classifications",
    "title": "",
    "section": "Section Classifications",
    "text": "Section Classifications\n\nsection_classifications &lt;- sections_gamma |&gt; \n  group_by(title, section) |&gt; \n  slice_max(gamma) |&gt; \n  ungroup()\n\nsection_classifications\n\n# A tibble: 112 × 4\n   title                   section topic gamma\n   &lt;chr&gt;                     &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1 The Communist Manifesto       1     4 1.00 \n 2 The Communist Manifesto       2     2 1.00 \n 3 The Communist Manifesto       3     1 1.00 \n 4 The Communist Manifesto       4     4 0.771\n 5 The Communist Manifesto       5     2 0.967\n 6 The Communist Manifesto       6     2 1.00 \n 7 The Red Record                1     3 1.00 \n 8 The Red Record                2     3 1.00 \n 9 The Red Record                3     3 0.727\n10 The Red Record                4     1 0.646\n# ℹ 102 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#book-topic-consensus",
    "href": "workshops/text_analysis.html#book-topic-consensus",
    "title": "",
    "section": "Book-Topic Consensus",
    "text": "Book-Topic Consensus\n\nbook_topics &lt;- section_classifications |&gt; \n  count(title, topic) |&gt; \n  group_by(title) |&gt; \n  slice_max(n, n = 1) |&gt;  \n  ungroup() |&gt; \n  transmute(consensus = title, topic)\n\nbook_topics\n\n# A tibble: 4 × 2\n  consensus                       topic\n  &lt;chr&gt;                           &lt;int&gt;\n1 The Communist Manifesto             2\n2 The Red Record                      3\n3 The Souls of Black Folk             3\n4 The Theory of the Leisure Class     2"
  },
  {
    "objectID": "workshops/text_analysis.html#incorrect-predictions-by-section",
    "href": "workshops/text_analysis.html#incorrect-predictions-by-section",
    "title": "",
    "section": "Incorrect Predictions by Section",
    "text": "Incorrect Predictions by Section\n\nsection_classifications |&gt; \n  inner_join(book_topics, by = \"topic\") |&gt; \n  filter(title != consensus) |&gt; \n  count(title, consensus)\n\n# A tibble: 4 × 3\n  title                           consensus                           n\n  &lt;chr&gt;                           &lt;chr&gt;                           &lt;int&gt;\n1 The Communist Manifesto         The Theory of the Leisure Class     3\n2 The Red Record                  The Souls of Black Folk            13\n3 The Souls of Black Folk         The Red Record                     35\n4 The Theory of the Leisure Class The Communist Manifesto            27"
  },
  {
    "objectID": "workshops/text_analysis.html#incorrect-predictions-by-word",
    "href": "workshops/text_analysis.html#incorrect-predictions-by-word",
    "title": "",
    "section": "Incorrect Predictions by Word",
    "text": "Incorrect Predictions by Word\n\nassignments &lt;- broom::augment(sections_lda, \n16                              data = sections_dtm)\nassignments\n\n\n16\n\nTakes our model and appends information to each observation in the original data\n\n\n\n\n\n\n# A tibble: 56,260 × 4\n   document                           term  count .topic\n   &lt;chr&gt;                              &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 The Theory of the Leisure Class_43 class    41      2\n 2 The Theory of the Leisure Class_45 class    37      2\n 3 The Souls of Black Folk_26         class     1      2\n 4 The Theory of the Leisure Class_26 class     4      1\n 5 The Communist Manifesto_3          class    16      1\n 6 The Theory of the Leisure Class_1  class    32      2\n 7 The Theory of the Leisure Class_51 class    32      2\n 8 The Theory of the Leisure Class_33 class    31      1\n 9 The Theory of the Leisure Class_28 class    30      2\n10 The Theory of the Leisure Class_30 class     5      1\n# ℹ 56,250 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#incorrect-predictions-by-word-1",
    "href": "workshops/text_analysis.html#incorrect-predictions-by-word-1",
    "title": "",
    "section": "Incorrect Predictions by Word",
    "text": "Incorrect Predictions by Word\n\n17assignments &lt;- assignments |&gt;\n  separate(document, c(\"title\", \"section\"),\n           sep = \"_\", convert = TRUE) |&gt;\n  inner_join(book_topics,\n             by = c(\".topic\" = \"topic\"))\n\nassignments\n\n\n17\n\nNow we can combine these data with book_topics to see which words were incorrectly classified\n\n\n\n\n\n\n# A tibble: 80,430 × 6\n   title                           section term  count .topic consensus         \n   &lt;chr&gt;                             &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;             \n 1 The Theory of the Leisure Class      43 class    41      2 The Communist Man…\n 2 The Theory of the Leisure Class      43 class    41      2 The Theory of the…\n 3 The Theory of the Leisure Class      45 class    37      2 The Communist Man…\n 4 The Theory of the Leisure Class      45 class    37      2 The Theory of the…\n 5 The Souls of Black Folk              26 class     1      2 The Communist Man…\n 6 The Souls of Black Folk              26 class     1      2 The Theory of the…\n 7 The Theory of the Leisure Class       1 class    32      2 The Communist Man…\n 8 The Theory of the Leisure Class       1 class    32      2 The Theory of the…\n 9 The Theory of the Leisure Class      51 class    32      2 The Communist Man…\n10 The Theory of the Leisure Class      51 class    32      2 The Theory of the…\n# ℹ 80,420 more rows"
  },
  {
    "objectID": "workshops/text_analysis.html#confusion-matrix",
    "href": "workshops/text_analysis.html#confusion-matrix",
    "title": "",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nCodeVisualization\n\n\n\nlibrary(scales)\n\nassignments |&gt; \n  count(title, consensus, wt = count) |&gt; \n  mutate(across(c(title, consensus), ~str_wrap(., 20))) |&gt; \n  group_by(title) |&gt; \n  mutate(percent = n / sum(n)) |&gt; \n  ggplot(aes(consensus, title, fill = percent)) +\n  geom_tile() +\n  scale_fill_gradient2(high = observable_light[4], label = percent_format()) +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        panel.grid = element_blank()) +\n  labs(x = \"Book words were assigned to\",\n       y = \"Book words came from\",\n       fill = \"% of assignments\")"
  },
  {
    "objectID": "workshops/text_analysis.html#two-topic-model",
    "href": "workshops/text_analysis.html#two-topic-model",
    "title": "",
    "section": "Two-Topic Model?",
    "text": "Two-Topic Model?\n\nTop TermsPer-Doc ClassificationAssignment Confusion Matrix"
  },
  {
    "objectID": "workshops/text_analysis.html#methods-and-implementation",
    "href": "workshops/text_analysis.html#methods-and-implementation",
    "title": "",
    "section": "Methods and Implementation",
    "text": "Methods and Implementation\n\n\nText Mining in R by Julia Silge & David Robinson\nSupervised Machine Learning for Text Analysis in R by Emil Hvitfeldt & Julia Silge\nJulia Silge’s blog\nText as Data by Justin Grimmer, Brandon M. Stewart, and Margaret E. Roberts\n\nCompanion course in R by Joe Ornstein\n\nMichael Clark’s Text Analysis Tutorial"
  },
  {
    "objectID": "workshops/text_analysis.html#uw-computing-resources",
    "href": "workshops/text_analysis.html#uw-computing-resources",
    "title": "",
    "section": "UW Computing Resources",
    "text": "UW Computing Resources\n\n\nCSDE Computing account\n\nCSDE SIM cluster\n\nResearch Computing Club (RCC) @ UW\n\nHyak - High Performance Computing (HPC)\n\nMore about Hyak can be found here\n\nCloud Credit Program"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Below is a list of all the courses for which I have served as a teaching assistant or instructor during my time at the University of Washington.\n\n\nInstructor\n\nSociology 221 Statistical Concepts and Methods for the Social Sciences - Summer 2021; Summer 2024\nCS&SS 508 Introduction to R for Social Scientists (Graduate Course) - Autumn 2023; Spring 2024\nSociology 316 Sociological Theory - Summer 2023\nSociology 404 Sociology in Practice: Community/Civic Internship Program - Spring 2018\nSociology 404 Sociology in Practice: Community/Civic Internship Program - Winter 2018\nSociology 321 Case-Based Social Statistics I (Honors Course) - Autumn 2017\n\n\n\nTeaching Assistant\n\nSociology 506 Methodology: Quantitative Techniques in Sociology (Graduate Course) - Spring 2021\nSociology 505 Applied Social Statistics (Graduate Course) - Winter 2021\nSociology 504 Applied Social Statistics (Graduate Course) - Autumn 2020\nStatistics 220 Basic Statistics - Summer 2015\nSociology 270 Social Problems - Spring 2015\nSociology 201 Surviving in South Africa: Contemporary Health and Population Issues - Winter 2015\nSociology 300 Foundations of Social Inquiry - Fall 2014"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#title-slide",
    "href": "workshops/data_wrangling_r.html#title-slide",
    "title": "",
    "section": "",
    "text": "Data Wrangling in R\nCSSCR Workshop\n16 October 2025\nVictoria Sass"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#roadmap",
    "href": "workshops/data_wrangling_r.html#roadmap",
    "title": "",
    "section": "Roadmap",
    "text": "Roadmap\n\nImporting and Exporting Data\nManipulating and Summarizing Data\nMerging Data\nTidying and Reshaping Data"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#data-packages",
    "href": "workshops/data_wrangling_r.html#data-packages",
    "title": "",
    "section": "Data Packages",
    "text": "Data Packages\nR has a big user base. If you are working with a popular data source, it will often have a devoted R package on CRAN or Github.\n\nExamples:\n\n\nWDI: World Development Indicators (World Bank)\n\ntidycensus: Census and American Community Survey\n\nquantmod: financial data from Yahoo, FRED, Google\n\ngssr: The General Social Survey Cumulative Data (1972-2021)\n\npsidR: Panel Study of Income Dynamics (basic & public datasets)\n\n\n\nIf you have an actual data file, you’ll have to import it yourself…"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#delimited-text-files",
    "href": "workshops/data_wrangling_r.html#delimited-text-files",
    "title": "",
    "section": "Delimited Text Files",
    "text": "Delimited Text Files\nBesides a package, it’s easiest when data is stored in a text file. The most commonly encountered delimited file is a .csv.\n\nA comma-separated values (.csv) file looks like the following:\n\"Subject\",\"Depression\",\"Sex\",\"Week\",\"HamD\",\"Imipramine\"\n101,\"Non-endogenous\",\"Second\",0,26,NA\n101,\"Non-endogenous\",\"Second\",1,22,NA\n101,\"Non-endogenous\",\"Second\",2,18,4.04305\n101,\"Non-endogenous\",\"Second\",3,7,3.93183\n101,\"Non-endogenous\",\"Second\",4,4,4.33073\n101,\"Non-endogenous\",\"Second\",5,3,4.36945\n103,\"Non-endogenous\",\"First\",0,33,NA\n103,\"Non-endogenous\",\"First\",1,24,NA\n103,\"Non-endogenous\",\"First\",2,15,2.77259"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#readr",
    "href": "workshops/data_wrangling_r.html#readr",
    "title": "",
    "section": "readr",
    "text": "readr\nR has some built-in functions for importing data, such as read.table() and read.csv().\n\nThe readr package provides similar functions, like read_csv(), that have slightly better features:\n\nFaster!\nBetter defaults (e.g. doesn’t automatically convert characters to factors)\nA bit smarter about dates and times\nLoading progress bars for large files\n\n\n\nreadr is one of the core tidyverse packages so loading tidyverse will load it too:\n\nlibrary(tidyverse)\n\n\n\nAlternatively, you can just load readr like so:\n\nlibrary(readr)"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#readr-importing-example",
    "href": "workshops/data_wrangling_r.html#readr-importing-example",
    "title": "",
    "section": "\nreadr Importing Example",
    "text": "readr Importing Example\nLet’s import some data about song ranks on the Billboard Hot 100 in 2000:\n\nbillboard_2000_raw &lt;- read_csv(file = \"data/billboard.csv\")\n\n\nHow do we know it loaded?\n\n\nLet’s look at it!\n\nglimpse(billboard_2000_raw)\n\n\n\nRows: 317\nColumns: 81\n$ year         &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200…\n$ artist       &lt;chr&gt; \"2 Pac\", \"2Ge+her\", \"3 Doors Down\", \"3 Doors Down\", \"504 …\n$ track        &lt;chr&gt; \"Baby Don't Cry (Keep...\", \"The Hardest Part Of ...\", \"Kr…\n$ time         &lt;time&gt; 04:22:00, 03:15:00, 03:53:00, 04:24:00, 03:35:00, 03:24:…\n$ date.entered &lt;date&gt; 2000-02-26, 2000-09-02, 2000-04-08, 2000-10-21, 2000-04-…\n$ wk1          &lt;dbl&gt; 87, 91, 81, 76, 57, 51, 97, 84, 59, 76, 84, 57, 50, 71, 7…\n$ wk2          &lt;dbl&gt; 82, 87, 70, 76, 34, 39, 97, 62, 53, 76, 84, 47, 39, 51, 6…\n$ wk3          &lt;dbl&gt; 72, 92, 68, 72, 25, 34, 96, 51, 38, 74, 75, 45, 30, 28, 5…\n$ wk4          &lt;dbl&gt; 77, NA, 67, 69, 17, 26, 95, 41, 28, 69, 73, 29, 28, 18, 4…\n$ wk5          &lt;dbl&gt; 87, NA, 66, 67, 17, 26, 100, 38, 21, 68, 73, 23, 21, 13, …\n$ wk6          &lt;dbl&gt; 94, NA, 57, 65, 31, 19, NA, 35, 18, 67, 69, 18, 19, 13, 3…\n$ wk7          &lt;dbl&gt; 99, NA, 54, 55, 36, 2, NA, 35, 16, 61, 68, 11, 20, 11, 34…\n$ wk8          &lt;dbl&gt; NA, NA, 53, 59, 49, 2, NA, 38, 14, 58, 65, 9, 17, 1, 29, …\n$ wk9          &lt;dbl&gt; NA, NA, 51, 62, 53, 3, NA, 38, 12, 57, 73, 9, 17, 1, 27, …\n$ wk10         &lt;dbl&gt; NA, NA, 51, 61, 57, 6, NA, 36, 10, 59, 83, 11, 17, 2, 30,…\n$ wk11         &lt;dbl&gt; NA, NA, 51, 61, 64, 7, NA, 37, 9, 66, 92, 1, 17, 2, 36, N…\n$ wk12         &lt;dbl&gt; NA, NA, 51, 59, 70, 22, NA, 37, 8, 68, NA, 1, 3, 3, 37, N…\n$ wk13         &lt;dbl&gt; NA, NA, 47, 61, 75, 29, NA, 38, 6, 61, NA, 1, 3, 3, 39, N…\n$ wk14         &lt;dbl&gt; NA, NA, 44, 66, 76, 36, NA, 49, 1, 67, NA, 1, 7, 4, 49, N…\n$ wk15         &lt;dbl&gt; NA, NA, 38, 72, 78, 47, NA, 61, 2, 59, NA, 4, 10, 12, 57,…\n$ wk16         &lt;dbl&gt; NA, NA, 28, 76, 85, 67, NA, 63, 2, 63, NA, 8, 17, 11, 63,…\n$ wk17         &lt;dbl&gt; NA, NA, 22, 75, 92, 66, NA, 62, 2, 67, NA, 12, 25, 13, 65…\n$ wk18         &lt;dbl&gt; NA, NA, 18, 67, 96, 84, NA, 67, 2, 71, NA, 22, 29, 15, 68…\n$ wk19         &lt;dbl&gt; NA, NA, 18, 73, NA, 93, NA, 83, 3, 79, NA, 23, 29, 18, 79…\n$ wk20         &lt;dbl&gt; NA, NA, 14, 70, NA, 94, NA, 86, 4, 89, NA, 43, 40, 20, 86…\n$ wk21         &lt;dbl&gt; NA, NA, 12, NA, NA, NA, NA, NA, 5, NA, NA, 44, 43, 30, NA…\n$ wk22         &lt;dbl&gt; NA, NA, 7, NA, NA, NA, NA, NA, 5, NA, NA, NA, 50, 40, NA,…\n$ wk23         &lt;dbl&gt; NA, NA, 6, NA, NA, NA, NA, NA, 6, NA, NA, NA, NA, 39, NA,…\n$ wk24         &lt;dbl&gt; NA, NA, 6, NA, NA, NA, NA, NA, 9, NA, NA, NA, NA, 44, NA,…\n$ wk25         &lt;dbl&gt; NA, NA, 6, NA, NA, NA, NA, NA, 13, NA, NA, NA, NA, NA, NA…\n$ wk26         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, 14, NA, NA, NA, NA, NA, NA…\n$ wk27         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, 16, NA, NA, NA, NA, NA, NA…\n$ wk28         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 23, NA, NA, NA, NA, NA, NA…\n$ wk29         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 22, NA, NA, NA, NA, NA, NA…\n$ wk30         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 33, NA, NA, NA, NA, NA, NA…\n$ wk31         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 36, NA, NA, NA, NA, NA, NA…\n$ wk32         &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, NA, 43, NA, NA, NA, NA, NA, NA…\n$ wk33         &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk34         &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk35         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk36         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk37         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk38         &lt;dbl&gt; NA, NA, 9, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk39         &lt;dbl&gt; NA, NA, 9, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk40         &lt;dbl&gt; NA, NA, 15, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk41         &lt;dbl&gt; NA, NA, 14, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk42         &lt;dbl&gt; NA, NA, 13, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk43         &lt;dbl&gt; NA, NA, 14, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk44         &lt;dbl&gt; NA, NA, 16, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk45         &lt;dbl&gt; NA, NA, 17, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk46         &lt;dbl&gt; NA, NA, 21, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk47         &lt;dbl&gt; NA, NA, 22, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk48         &lt;dbl&gt; NA, NA, 24, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk49         &lt;dbl&gt; NA, NA, 28, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk50         &lt;dbl&gt; NA, NA, 33, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk51         &lt;dbl&gt; NA, NA, 42, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk52         &lt;dbl&gt; NA, NA, 42, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk53         &lt;dbl&gt; NA, NA, 49, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk54         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk55         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk56         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk57         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk58         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk59         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk60         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk61         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk62         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk63         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk64         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk65         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk66         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk67         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk68         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk69         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk70         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk71         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk72         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk73         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk74         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk75         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk76         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#alternate-solution",
    "href": "workshops/data_wrangling_r.html#alternate-solution",
    "title": "",
    "section": "Alternate Solution",
    "text": "Alternate Solution\nWhen you import data from an external file you’ll also see it in the Global Environment tab in the upper-right pane of RStudio:\n\n\n\nYou can also import the data manually!\nIn the upper right-hand pane of RStudio (make sure you’re in the Environment tab), select:\nImport Dataset &gt; From Text (readr) and browse to the file on your computer1.\n\n\nOnce you’ve imported the data, you can copy/paste the import code from the console into your file!!\nThis makes the process reproducible!\n\n\n\n\nIdeally you’ve saved it in your project folder!"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#manual-data-import",
    "href": "workshops/data_wrangling_r.html#manual-data-import",
    "title": "",
    "section": "Manual Data Import",
    "text": "Manual Data Import"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#specifying-nas",
    "href": "workshops/data_wrangling_r.html#specifying-nas",
    "title": "",
    "section": "Specifying NAs",
    "text": "Specifying NAs\nSometimes a particular dataset or file read from a different software will code NAs differently than R. If that’s the case, you can add additional specifications to read_csv for what to read in as NA.\n\nbillboard_2000_raw &lt;- read_csv(file = \"data/billboard.csv\", \n                               na = c(\"N/A\", \"999\"))"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#skipping-lines",
    "href": "workshops/data_wrangling_r.html#skipping-lines",
    "title": "",
    "section": "Skipping lines",
    "text": "Skipping lines\nDepending on how the data were input, there may be several lines that precede the beginning of the data table you’re interested in importing. You can skip these lines of metadata with the skip argument:\n\nbillboard_2000_raw &lt;- read_csv(file = \"data/billboard.csv\", \n                               skip = 1)"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#variable-names",
    "href": "workshops/data_wrangling_r.html#variable-names",
    "title": "",
    "section": "Variable names",
    "text": "Variable names\nread_csv will automatically take the first row as column names. If you want to rename them you can save yourself some time recoding later on if you specify your preferred variable names upfront with the col_names argument.\n\nIt takes a character vector to be used as column names (in their order of appearance).\n\nbillboard_renamed &lt;- read_csv(file = \"data/billboard.csv\",\n                              col_names = c(\"year\", \"artist\", \"track\", \"time\", \"date_entered\", \n1                                            paste(\"wk\", 1:76, sep = \"_\")))\n\n2billboard_renamed |&gt;  names() |&gt; head(10)\n\n\n1\n\nFirst few entries: “wk_1” “wk_2” “wk_3”\n\n2\n\nnames returns the column names.\n\n\n\n\n [1] \"year\"         \"artist\"       \"track\"        \"time\"         \"date_entered\"\n [6] \"wk_1\"         \"wk_2\"         \"wk_3\"         \"wk_4\"         \"wk_5\"        \n\n\n\n\nIf you don’t have any variable names you can specify that instead.\n\nbillboard_2000_raw &lt;- read_csv(file = \"data/billboard.csv\", \n                               col_names = FALSE)"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#snake-case",
    "href": "workshops/data_wrangling_r.html#snake-case",
    "title": "",
    "section": "Snake Case",
    "text": "Snake Case\nIf you simply want to change your variables to snake case (all lower case; words separated by _), you can use the function clean_names() from the janitor package which replaces other punctuation separators with _.\n\n# Download package first\n3# install.packages(\"janitor\")\n\n# Create new object for renamed data\nbillboard_renamed &lt;- billboard_2000_raw |&gt; \n4  janitor::clean_names(numerals = \"right\")\n\nbillboard_renamed |&gt;  names() |&gt; head(10)\n\n\n3\n\nRun in the console first.\n\n4\n\nYou can call a function without loading its package by specifying its package name followed by :: before it;  The numerals argument specifies if you additionally want to put a separator before a number.\n\n\n\n\n\n\n [1] \"year\"         \"artist\"       \"track\"        \"time\"         \"date_entered\"\n [6] \"wk_1\"         \"wk_2\"         \"wk_3\"         \"wk_4\"         \"wk_5\""
  },
  {
    "objectID": "workshops/data_wrangling_r.html#other-data-file-types-with-readr",
    "href": "workshops/data_wrangling_r.html#other-data-file-types-with-readr",
    "title": "",
    "section": "Other Data File Types with readr\n",
    "text": "Other Data File Types with readr\n\nThe other functions in readr employ a similar approach to read_csv so the trick is just knowing which to use for what data type.\n\n\n\nread_csv2 is separated by semicolons (instead of commas)\n\nread_tsv is separated by tabs\n\nread_delim guesses the delimiter\n\nread_fwf reads in fixed-width-files\n\nread_table is a variation of fwf where columns are separated by white space\n\nread_log reads in Apache-style log files"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#other-packages-to-read-in-data",
    "href": "workshops/data_wrangling_r.html#other-packages-to-read-in-data",
    "title": "",
    "section": "Other Packages to Read in Data",
    "text": "Other Packages to Read in Data\nThere are a range of other ways, besides delimited files, that data are stored.\nThe following packages are part of the extended tidyverse and therefore operate with similar syntax and logic as readr."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#other-packages-to-read-in-data-1",
    "href": "workshops/data_wrangling_r.html#other-packages-to-read-in-data-1",
    "title": "",
    "section": "Other Packages to Read in Data",
    "text": "Other Packages to Read in Data\nThere are a range of other ways, besides delimited files, that data are stored.\nThe following packages are part of the extended tidyverse and therefore operate with similar syntax and logic as readr.\n\n\n\n\n\nFor Excel files (.xls or .xlsx), use package readxl1\n\n\n\n\n\nNote: For Excel files and Googlesheets You won’t keep text formatting, color, comments, or merged cells. See the openxlsx package for those capabilities. Also, tidyxl can help import non-tabular data from Excel.\nFunctions have additional arguments to read in specific sheets or a range of cells."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#other-packages-to-read-in-data-2",
    "href": "workshops/data_wrangling_r.html#other-packages-to-read-in-data-2",
    "title": "",
    "section": "Other Packages to Read in Data",
    "text": "Other Packages to Read in Data\nThere are a range of other ways, besides delimited files, that data are stored.\nThe following packages are part of the extended tidyverse and therefore operate with similar syntax and logic as readr.\n\n\n\n\n\nFor Excel files (.xls or .xlsx), use package readxl1\n\nFor Google Docs Spreadsheets, use package googlesheets42\n\n\n\n\n\nNote: For Excel files and Googlesheets You won’t keep text formatting, color, comments, or merged cells. See the openxlsx package for those capabilities. Also, tidyxl can help import non-tabular data from Excel.\nFunctions have additional arguments to read in specific sheets or a range of cells.Very similar to readxl with some slight variations you can read about here."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#other-packages-to-read-in-data-3",
    "href": "workshops/data_wrangling_r.html#other-packages-to-read-in-data-3",
    "title": "",
    "section": "Other Packages to Read in Data",
    "text": "Other Packages to Read in Data\nThere are a range of other ways, besides delimited files, that data are stored.\nThe following packages are part of the extended tidyverse and therefore operate with similar syntax and logic as readr.\n\n\n\n\n\nFor Excel files (.xls or .xlsx), use package readxl1\n\nFor Google Docs Spreadsheets, use package googlesheets42\n\nFor Stata, SPSS, and SAS files, use package haven3\n\n\n\n\n\nNote: For Excel files and Googlesheets You won’t keep text formatting, color, comments, or merged cells. See the openxlsx package for those capabilities. Also, tidyxl can help import non-tabular data from Excel.\nFunctions have additional arguments to read in specific sheets or a range of cells.Very similar to readxl with some slight variations you can read about here.SAS, SPSS, and Stata have so-called “labelled” vectors for which haven provides a class to represent in R. Alternatively, you can get rid of them with these functions."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#how-does-readr-parse-different-data-types",
    "href": "workshops/data_wrangling_r.html#how-does-readr-parse-different-data-types",
    "title": "",
    "section": "How does readr parse different data types?",
    "text": "How does readr parse different data types?\nFor each column in a data frame, readr functions pull the first 1000 rows and checks:\n\n\n\n\n\nflowchart LR\n    id1((Variable))==&gt;A([\"1. Does it contain only F, T, FALSE, TRUE, or NA (ignoring case)?\"])==&gt;id2{{Logical}}\n    id1((Variable))==&gt;B([\"2. Does it contain only numbers (e.g., 1, -4.5, 5e6, Inf?)\"])==&gt;id3{{Number}}\n    id1((Variable))==&gt;C([\"3. Does it match the ISO8601 standard?\"])==&gt;id4{{Date/Date-time}}\n    id1((Variable))==&gt;D([\"4. None of the above\"])==&gt;id5{{String}}\n    style id1 fill:#4B2E83,color:#B7A57A,stroke:#B7A57A\n    style id2 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id3 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id4 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id5 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style A fill:#FFFFFF,color:#000000,stroke:#000000\n    style B fill:#FFFFFF,color:#000000,stroke:#000000\n    style C fill:#FFFFFF,color:#000000,stroke:#000000\n    style D fill:#FFFFFF,color:#000000,stroke:#000000"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#how-does-readr-parse-different-data-types-1",
    "href": "workshops/data_wrangling_r.html#how-does-readr-parse-different-data-types-1",
    "title": "",
    "section": "How does readr parse different data types?",
    "text": "How does readr parse different data types?\nFor each column in a data frame, readr functions pull the first 1000 rows and checks:\n\n\n\n\n\nflowchart LR\n    id1((Variable))==&gt;A([\"1. Does it contain only F, T, FALSE, TRUE, or NA (ignoring case)?\"])==&gt;id2{{Logical}}\n    id1((Variable))==&gt;B([\"2. Does it contain only numbers (e.g., 1, -4.5, 5e6, Inf?)\"])==&gt;id3{{Number}}\n    id1((Variable))==&gt;C([\"3. Does it match the ISO8601 standard?\"])==&gt;id4{{Date/Date-time}}\n    id1((Variable))==&gt;D([\"4. None of the above\"])==&gt;id5{{String}}\n    style id1 fill:#4B2E83,color:#B7A57A,stroke:#B7A57A\n    style id2 fill:#4B2E83,color:#B7A57A,stroke:#B7A57A\n    style id3 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id4 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id5 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style A fill:#B7A57A,color:#000000,stroke:#000000\n    style B fill:#FFFFFF,color:#000000,stroke:#000000\n    style C fill:#FFFFFF,color:#000000,stroke:#000000\n    style D fill:#FFFFFF,color:#000000,stroke:#000000"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#how-does-readr-parse-different-data-types-2",
    "href": "workshops/data_wrangling_r.html#how-does-readr-parse-different-data-types-2",
    "title": "",
    "section": "How does readr parse different data types?",
    "text": "How does readr parse different data types?\nFor each column in a data frame, readr functions pull the first 1000 rows and checks:\n\n\n\n\n\nflowchart LR\n    id1((Variable))==&gt;A([\"1. Does it contain only F, T, FALSE, TRUE, or NA (ignoring case)?\"])==&gt;id2{{Logical}}\n    id1((Variable))==&gt;B([\"2. Does it contain only numbers (e.g., 1, -4.5, 5e6, Inf?)\"])==&gt;id3{{Number}}\n    id1((Variable))==&gt;C([\"3. Does it match the ISO8601 standard?\"])==&gt;id4{{Date/Date-time}}\n    id1((Variable))==&gt;D([\"4. None of the above\"])==&gt;id5{{String}}\n    style id1 fill:#4B2E83,color:#B7A57A,stroke:#B7A57A\n    style id2 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id3 fill:#4B2E83,color:#B7A57A,stroke:#B7A57A\n    style id4 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id5 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style A fill:#FFFFFF,color:#000000,stroke:#000000\n    style B fill:#B7A57A,color:#000000,stroke:#000000\n    style C fill:#FFFFFF,color:#000000,stroke:#000000\n    style D fill:#FFFFFF,color:#000000,stroke:#000000"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#how-does-readr-parse-different-data-types-3",
    "href": "workshops/data_wrangling_r.html#how-does-readr-parse-different-data-types-3",
    "title": "",
    "section": "How does readr parse different data types?",
    "text": "How does readr parse different data types?\nFor each column in a data frame, readr functions pull the first 1000 rows and checks:\n\n\n\n\n\nflowchart LR\n    id1((Variable))==&gt;A([\"1. Does it contain only F, T, FALSE, TRUE, or NA (ignoring case)?\"])==&gt;id2{{Logical}}\n    id1((Variable))==&gt;B([\"2. Does it contain only numbers (e.g., 1, -4.5, 5e6, Inf?)\"])==&gt;id3{{Number}}\n    id1((Variable))==&gt;C([\"3. Does it match the ISO8601 standard?\"])==&gt;id4{{Date/Date-time}}\n    id1((Variable))==&gt;D([\"4. None of the above\"])==&gt;id5{{String}}\n    style id1 fill:#4B2E83,color:#B7A57A,stroke:#B7A57A\n    style id2 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id3 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id4 fill:#4B2E83,color:#B7A57A,stroke:#B7A57A\n    style id5 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style A fill:#FFFFFF,color:#000000,stroke:#000000\n    style B fill:#FFFFFF,color:#000000,stroke:#000000\n    style C fill:#B7A57A,color:#000000,stroke:#000000\n    style D fill:#FFFFFF,color:#000000,stroke:#000000"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#how-does-readr-parse-different-data-types-4",
    "href": "workshops/data_wrangling_r.html#how-does-readr-parse-different-data-types-4",
    "title": "",
    "section": "How does readr parse different data types?",
    "text": "How does readr parse different data types?\nFor each column in a data frame, readr functions pull the first 1000 rows and checks:\n\n\n\n\n\nflowchart LR\n    id1((Variable))==&gt;A([\"1. Does it contain only F, T, FALSE, TRUE, or NA (ignoring case)?\"])==&gt;id2{{Logical}}\n    id1((Variable))==&gt;B([\"2. Does it contain only numbers (e.g., 1, -4.5, 5e6, Inf?)\"])==&gt;id3{{Number}}\n    id1((Variable))==&gt;C([\"3. Does it match the ISO8601 standard?\"])==&gt;id4{{Date/Date-time}}\n    id1((Variable))==&gt;D([\"4. None of the above\"])==&gt;id5{{String}}\n    style id1 fill:#4B2E83,color:#B7A57A,stroke:#B7A57A\n    style id2 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id3 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id4 fill:#B7A57A,color:#4B2E83,stroke:#4B2E83\n    style id5 fill:#4B2E83,color:#B7A57A,stroke:#B7A57A\n    style A fill:#FFFFFF,color:#000000,stroke:#000000\n    style B fill:#FFFFFF,color:#000000,stroke:#000000\n    style C fill:#FFFFFF,color:#000000,stroke:#000000\n    style D fill:#B7A57A,color:#000000,stroke:#000000"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#most-common-issue-with-reading-in-data",
    "href": "workshops/data_wrangling_r.html#most-common-issue-with-reading-in-data",
    "title": "",
    "section": "Most Common Issue with Reading in Data",
    "text": "Most Common Issue with Reading in Data\nThe most common problem that occurs when reading in data is having mixed data. Most often, given the heuristic provided in the last slide, R will parse a variable as a character string to preserve whatever it contains.\n\nLet’s actually look at how the billboard data was read in:\n\nglimpse(billboard_2000_raw) \n\n\n\nRows: 317\nColumns: 81\n$ year         &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200…\n$ artist       &lt;chr&gt; \"2 Pac\", \"2Ge+her\", \"3 Doors Down\", \"3 Doors Down\", \"504 …\n$ track        &lt;chr&gt; \"Baby Don't Cry (Keep...\", \"The Hardest Part Of ...\", \"Kr…\n$ time         &lt;time&gt; 04:22:00, 03:15:00, 03:53:00, 04:24:00, 03:35:00, 03:24:…\n$ date.entered &lt;date&gt; 2000-02-26, 2000-09-02, 2000-04-08, 2000-10-21, 2000-04-…\n$ wk1          &lt;dbl&gt; 87, 91, 81, 76, 57, 51, 97, 84, 59, 76, 84, 57, 50, 71, 7…\n$ wk2          &lt;dbl&gt; 82, 87, 70, 76, 34, 39, 97, 62, 53, 76, 84, 47, 39, 51, 6…\n$ wk3          &lt;dbl&gt; 72, 92, 68, 72, 25, 34, 96, 51, 38, 74, 75, 45, 30, 28, 5…\n$ wk4          &lt;dbl&gt; 77, NA, 67, 69, 17, 26, 95, 41, 28, 69, 73, 29, 28, 18, 4…\n$ wk5          &lt;dbl&gt; 87, NA, 66, 67, 17, 26, 100, 38, 21, 68, 73, 23, 21, 13, …\n$ wk6          &lt;dbl&gt; 94, NA, 57, 65, 31, 19, NA, 35, 18, 67, 69, 18, 19, 13, 3…\n$ wk7          &lt;dbl&gt; 99, NA, 54, 55, 36, 2, NA, 35, 16, 61, 68, 11, 20, 11, 34…\n$ wk8          &lt;dbl&gt; NA, NA, 53, 59, 49, 2, NA, 38, 14, 58, 65, 9, 17, 1, 29, …\n$ wk9          &lt;dbl&gt; NA, NA, 51, 62, 53, 3, NA, 38, 12, 57, 73, 9, 17, 1, 27, …\n$ wk10         &lt;dbl&gt; NA, NA, 51, 61, 57, 6, NA, 36, 10, 59, 83, 11, 17, 2, 30,…\n$ wk11         &lt;dbl&gt; NA, NA, 51, 61, 64, 7, NA, 37, 9, 66, 92, 1, 17, 2, 36, N…\n$ wk12         &lt;dbl&gt; NA, NA, 51, 59, 70, 22, NA, 37, 8, 68, NA, 1, 3, 3, 37, N…\n$ wk13         &lt;dbl&gt; NA, NA, 47, 61, 75, 29, NA, 38, 6, 61, NA, 1, 3, 3, 39, N…\n$ wk14         &lt;dbl&gt; NA, NA, 44, 66, 76, 36, NA, 49, 1, 67, NA, 1, 7, 4, 49, N…\n$ wk15         &lt;dbl&gt; NA, NA, 38, 72, 78, 47, NA, 61, 2, 59, NA, 4, 10, 12, 57,…\n$ wk16         &lt;dbl&gt; NA, NA, 28, 76, 85, 67, NA, 63, 2, 63, NA, 8, 17, 11, 63,…\n$ wk17         &lt;dbl&gt; NA, NA, 22, 75, 92, 66, NA, 62, 2, 67, NA, 12, 25, 13, 65…\n$ wk18         &lt;dbl&gt; NA, NA, 18, 67, 96, 84, NA, 67, 2, 71, NA, 22, 29, 15, 68…\n$ wk19         &lt;dbl&gt; NA, NA, 18, 73, NA, 93, NA, 83, 3, 79, NA, 23, 29, 18, 79…\n$ wk20         &lt;dbl&gt; NA, NA, 14, 70, NA, 94, NA, 86, 4, 89, NA, 43, 40, 20, 86…\n$ wk21         &lt;dbl&gt; NA, NA, 12, NA, NA, NA, NA, NA, 5, NA, NA, 44, 43, 30, NA…\n$ wk22         &lt;dbl&gt; NA, NA, 7, NA, NA, NA, NA, NA, 5, NA, NA, NA, 50, 40, NA,…\n$ wk23         &lt;dbl&gt; NA, NA, 6, NA, NA, NA, NA, NA, 6, NA, NA, NA, NA, 39, NA,…\n$ wk24         &lt;dbl&gt; NA, NA, 6, NA, NA, NA, NA, NA, 9, NA, NA, NA, NA, 44, NA,…\n$ wk25         &lt;dbl&gt; NA, NA, 6, NA, NA, NA, NA, NA, 13, NA, NA, NA, NA, NA, NA…\n$ wk26         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, 14, NA, NA, NA, NA, NA, NA…\n$ wk27         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, 16, NA, NA, NA, NA, NA, NA…\n$ wk28         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 23, NA, NA, NA, NA, NA, NA…\n$ wk29         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 22, NA, NA, NA, NA, NA, NA…\n$ wk30         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 33, NA, NA, NA, NA, NA, NA…\n$ wk31         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 36, NA, NA, NA, NA, NA, NA…\n$ wk32         &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, NA, 43, NA, NA, NA, NA, NA, NA…\n$ wk33         &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk34         &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk35         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk36         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk37         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk38         &lt;dbl&gt; NA, NA, 9, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk39         &lt;dbl&gt; NA, NA, 9, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk40         &lt;dbl&gt; NA, NA, 15, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk41         &lt;dbl&gt; NA, NA, 14, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk42         &lt;dbl&gt; NA, NA, 13, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk43         &lt;dbl&gt; NA, NA, 14, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk44         &lt;dbl&gt; NA, NA, 16, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk45         &lt;dbl&gt; NA, NA, 17, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk46         &lt;dbl&gt; NA, NA, 21, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk47         &lt;dbl&gt; NA, NA, 22, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk48         &lt;dbl&gt; NA, NA, 24, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk49         &lt;dbl&gt; NA, NA, 28, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk50         &lt;dbl&gt; NA, NA, 33, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk51         &lt;dbl&gt; NA, NA, 42, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk52         &lt;dbl&gt; NA, NA, 42, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk53         &lt;dbl&gt; NA, NA, 49, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk54         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk55         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk56         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk57         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk58         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk59         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk60         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk61         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk62         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk63         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk64         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk65         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk66         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk67         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk68         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk69         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk70         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk71         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk72         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk73         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk74         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk75         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk76         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#what-went-wrong",
    "href": "workshops/data_wrangling_r.html#what-went-wrong",
    "title": "",
    "section": "What Went Wrong?",
    "text": "What Went Wrong?\nSince readr uses the values in the first 1000 rows to guess the type of the column (logical, numeric, date/date-time, character), if the first 1000 rows don’t have any data, they will be coded as logical variables.\n\nThere are not many songs in the data that charted for 60+ weeks—and none in the first 1000 that charted for 66+ weeks!\n\n\n\n\n\n  NA is logical?\n\n\n\nclass(c(T, F, NA, FALSE, TRUE))\n5class(c(1, NA, 17.5, 5.3, NA))\n6class(as.Date(c(NA, \"2023-10-31\", \"1986-06-21\", \"1997-01-15\"), tz = \"America/Los_Angeles\"))\nclass(c(\"apple\", NA, \"mango\", \"blackberry\", \"plum\")) \nclass(c(NA, NA, NA, NA, NA))\n\n\n5\n\nclass returns the data type of its first argument.\n\n6\n\nas.Date turns a character string of dates into an official date class in Base R. If we had an accompanying time stamp we would need to use as.POSIXct which turns a character string of dates and times into an official date-time class in Base R.\n\n\n\n\n\n\n[1] \"logical\"\n[1] \"numeric\"\n[1] \"Date\"\n[1] \"character\"\n[1] \"logical\"\n\n\n\n\n\n\n\n\nTechnically, NAs can be any data type depending upon what they are grouped with. However, by themselves they are a logical indicator of missing data, so their class is logical."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#column-types",
    "href": "workshops/data_wrangling_r.html#column-types",
    "title": "",
    "section": "Column types",
    "text": "Column types\nSince the wk* variables should all be read in as integers, we can specify this explicitly with the col_types argument.\n\n\n# Create character string of shortcode column types\n7bb_types &lt;- paste(c(\"icctD\", rep(\"i\", 76)), collapse=\"\")\nbb_types \n\n\n7\n\nYou can short-code column types with i = integer, c = character, t = time, D = date.  The collapse argument collapses the first two arguments into one complete character string.\n\n\n\n\n\n\n[1] \"icctDiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\"\n\n\n\n\n\n\n# re-read in data with column types specified\nbillboard_2000_raw &lt;- read_csv(file = \"data/billboard.csv\", \n8                               col_types = bb_types)\n\n\n8\n\nSee all column types and short codes here."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#column-types-1",
    "href": "workshops/data_wrangling_r.html#column-types-1",
    "title": "",
    "section": "Column types",
    "text": "Column types\nTo specify a default column type you can use .default like so:\n\nbillboard_2000_raw &lt;- read_csv(file = \"data/billboard.csv\", \n                               col_types = cols(.default = col_character())) \n\n\n\nAnother useful helper is cols_only() for when you only want to read in a subset of all available variables.\n\nbillboard_2000_raw &lt;- read_csv(file = \"data/billboard.csv\", \n                               col_types = cols_only(x = col_character)) \n\n\n\n\nIn summary, the col_types argument gives you greater control over how your data are read in and can save you recoding time down the road and/or point out where your data are behaving differently than you expect."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#writing-delimited-files",
    "href": "workshops/data_wrangling_r.html#writing-delimited-files",
    "title": "",
    "section": "Writing Delimited Files",
    "text": "Writing Delimited Files\nGetting data out of R into a delimited file is very similar to getting it into R:\n\nwrite_csv(billboard_2000_raw, path = \"data/billboard_data.csv\")\n\nThis saved the data we pulled off the web in a file called billboard_data.csv in the data folder of my working directory.\n\nHowever, saving data in this way will not preserve R data types since delimited files code everything as a character string.\n\n\nTo save R objects and all associated metadata you have two options:\n\n\n.Rds format:\n.Rdata or .Rda format:\n\n\n\n\nUsed for single objects, doesn’t save the original object name\nSave: write_rds(old_object_name, \"path.Rds\")\n\nLoad: new_object_name &lt;- read_rds(\"path.Rds\")\n\n\n\n\n\nUsed for saving multiple files where the original object names are preserved\nSave: save(object1, object2, ... , file = \"path.Rdata\")\n\nLoad: load(\"path.Rdata\") without assignment operator"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#writing-other-file-types",
    "href": "workshops/data_wrangling_r.html#writing-other-file-types",
    "title": "",
    "section": "Writing Other File-Types",
    "text": "Writing Other File-Types\n\n\nwritexl\ngooglesheets4\nhaven\n\n\n\n\n\n\n\nwrite_xlsx() writes to an xlsx file\n\n\n\n\n\n\n\n\n\n\nsheet_write() or write_sheet() (over)writes new data into a Sheet\n\ngs4_create() creates a new Sheet\n\nsheet_append() appends rows to a sheet\n\nrange_write() (over)writes new data into a range\n\nrange_flood() floods a range of cells\n`range_clear() clears a range of cells\n\n\n\n\n\n\n\n\n\n\nwrite_dta() writes Stata DTA files\n\nwrite_sav() writes SPSS files\n\nwrite_xpt() writes SAS transport files"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#death-to-spreadsheets",
    "href": "workshops/data_wrangling_r.html#death-to-spreadsheets",
    "title": "",
    "section": "Death to Spreadsheets",
    "text": "Death to Spreadsheets\nTools like Excel or Google Sheets let you manipulate spreadsheets using functions.\n\nSpreadsheets are not reproducible: It’s hard to know how someone changed the raw data!\nIt’s hard to catch mistakes when you use spreadsheets1.\n\n\nWe want to know how to use R to manipulate data more transparently and reproducibly.\n\nDon’t be the next sad Research Assistant who makes headlines with an Excel error! (Reinhart & Rogoff, 2010)"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#data-types-in-r",
    "href": "workshops/data_wrangling_r.html#data-types-in-r",
    "title": "",
    "section": "Data types in R\n",
    "text": "Data types in R\n\nThere are a variety of data types in R:\n\n\nFactors\nDate/Date-time\nLogical\nNumeric\nMissing Values\nStrings"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#data-types-in-r-1",
    "href": "workshops/data_wrangling_r.html#data-types-in-r-1",
    "title": "",
    "section": "Data types in R\n",
    "text": "Data types in R\n\nThere are a variety of data types in R:\n\nFactors\nDate/Date-time\nLogical\nNumeric\nMissing Values\nStrings"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#logical-operators-in-r",
    "href": "workshops/data_wrangling_r.html#logical-operators-in-r",
    "title": "",
    "section": "Logical Operators in R\n",
    "text": "Logical Operators in R\n\nComparing objects\n\n\n\n\n\n==:\n\n!=:\n\n&gt;, &gt;=, &lt;, &lt;=:\n\n%in%:\n\n\n\n\n\nis equal to1\n\nnot equal to\nless than, less than or equal to, etc.\nused when checking if equal to one of several values\n\n\n\n\nCombining comparisons\n\n\n\n\n\n\n&:\n\n|:\n\n!:\n\nxor():\n\n\n\n\n\n\nboth conditions need to hold (AND)\n\n\nat least one condition needs to hold (OR)\n\n\ninverts a logical condition (TRUE becomes FALSE, vice versa)\n\n\nexclusive OR (i.e. x or y but NOT both)\n\n\n\n\n\nYou may also see && and || but they are what’s known as short-circuiting operators and are not to be used in dplyr functions (used for programming not data manipulation); they’ll only ever return a single TRUE or FALSE.\nNote: there are TWO equal signs here!"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#logical-summaries",
    "href": "workshops/data_wrangling_r.html#logical-summaries",
    "title": "",
    "section": "Logical Summaries",
    "text": "Logical Summaries\n\n\n\n\n\nany():\n\nall():\n\n\n\n\n\nthe equivalent of |; it’ll return TRUE if there are any TRUE’s in x\nthe equivalent of &; it’ll return TRUE only if all values of x are TRUE’s\n\n\n\n\n\nC &lt;- c(5, 10, NA, 10, 20, NA)\nany(C &lt;= 10) \n1all(C &lt;= 20)\n2all(C &lt;= 20, na.rm = TRUE)\n3mean(C, na.rm = TRUE)\n\n\n1\n\nLike other summary functions, they’ll return NA if there are any missing values present and it’s FALSE.\n\n2\n\nUse na.rm = TRUE to remove NAs prior to evaluation.\n\n3\n\nWhen you evaluate a logical vector numerically, TRUE = 1 and FALSE = 0. This makes sum() and mean() useful when summarizing logical functions (sum gives number of TRUEs and mean gives the proportion).\n\n\n\n\n[1] TRUE\n[1] NA\n[1] TRUE\n[1] 11.25"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#conditional-transformations",
    "href": "workshops/data_wrangling_r.html#conditional-transformations",
    "title": "",
    "section": "Conditional transformations",
    "text": "Conditional transformations\n\nif_else()\n\nIf you want to use one value when a condition is TRUE and another value when it’s FALSE.\n\n\nif_else(condition = \"A logical vector\", \n        true = \"Output when condition is true\", \n        false = \"Output when condition is false\")\n\n\n\n\nx &lt;- c(-3:3, NA)\n4if_else(x &gt; 0, \"+ve\", \"-ve\", \"???\")\n\n\n4\n\nThere’s an optional fourth argument, missing which will be used if the input is NA.\n\n\n\n\n[1] \"-ve\" \"-ve\" \"-ve\" \"-ve\" \"+ve\" \"+ve\" \"+ve\" \"???\"\n\n\n\n\n\ncase_when()\n\nA very useful extension of if_else() for multiple conditions1.\n\n\n\ncase_when(\n  x == 0   ~ \"0\",\n  x &lt; 0    ~ \"-ve\", \n  x &gt; 0    ~ \"+ve\",\n5  is.na(x) ~ \"???\"\n6)\n\n\n5\n\nUse .default if you want to create a “default”/catch all value.\n\n6\n\nBoth functions require compatible types: i.e. numerical and logical, strings and factors, dates and datetimes, NA and everything.\n\n\n\n\n[1] \"-ve\" \"-ve\" \"-ve\" \"0\"   \"+ve\" \"+ve\" \"+ve\" \"???\"\n\n\n\nNote that if multiple conditions match in case_when(), only the first will be used."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#dplyr",
    "href": "workshops/data_wrangling_r.html#dplyr",
    "title": "",
    "section": "dplyr",
    "text": "dplyr\nToday, we’ll use tools from the dplyr package to manipulate data!\n\n\ndplyr is part of the Tidyverse, and included in the tidyverse package.\n\n\nlibrary(tidyverse)\n\n\nTo demonstrate data transformations we’re going to use the nycflights13 dataset, which you’ll need to download and load into R\n\n# Download and load data\n7# install.packages(\"nycflights13\")\n8library(nycflights13)\n\n\n7\n\nRun in console.\n\n8\n\nLoad into R session.\n\n\n\n\n\n\nnycflights13 includes five dataframes1, some of which contain missing data (NA):\n\n9data(flights)\n10data(airlines)\n11data(airports)\n12data(planes)\n13data(weather)\n\n\n9\n\nflights leaving JFK, LGA, or EWR in 2013\n\n10\n\nairline abbreviations\n\n11\n\nairport metadata\n\n12\n\nairplane metadata\n\n13\n\nhourly weather data for JFK, LGA, and EWR\n\n\n\n\n\nNote these are separate data frames, each needing to be loaded separately:"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#dplyr-basics",
    "href": "workshops/data_wrangling_r.html#dplyr-basics",
    "title": "",
    "section": "\ndplyr Basics",
    "text": "dplyr Basics\nAll dplyr functions have the following in common:\n\nThe first argument is always a data frame.\nThe subsequent arguments typically describe which columns to operate on, using the variable names (without quotes).\nThe output is always a new data frame.\n\n\nEach function operates either on rows, columns, groups, or entire tables.\n\n\nTo save the transformations you’ve made to a data frame you’ll need to save the output to a new object."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#subset-rows-filter",
    "href": "workshops/data_wrangling_r.html#subset-rows-filter",
    "title": "",
    "section": "Subset Rows: filter()\n",
    "text": "Subset Rows: filter()\n\nWe often get big datasets, and we only want some of the entries. We can subset rows using filter().\n\n\ndelay_2hr &lt;- flights |&gt; \n14  filter(dep_delay &gt; 120)\n15delay_2hr\n\n\n14\n\nHere’s where we’ll use a lot of logical operators. Make sure to use == not = to test the logical condition.\n\n15\n\nNow, delay_2hr is an object in our environment which contains rows corresponding to flights that experienced at least a 2 hour delay.\n\n\n\n\n# A tibble: 9,723 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      848           1835       853     1001           1950\n 2  2013     1     1      957            733       144     1056            853\n 3  2013     1     1     1114            900       134     1447           1222\n 4  2013     1     1     1540           1338       122     2020           1825\n 5  2013     1     1     1815           1325       290     2120           1542\n 6  2013     1     1     1842           1422       260     1958           1535\n 7  2013     1     1     1856           1645       131     2212           2005\n 8  2013     1     1     1934           1725       129     2126           1855\n 9  2013     1     1     1938           1703       155     2109           1823\n10  2013     1     1     1942           1705       157     2124           1830\n# ℹ 9,713 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#subset-columns-select",
    "href": "workshops/data_wrangling_r.html#subset-columns-select",
    "title": "",
    "section": "Subset Columns: select()\n",
    "text": "Subset Columns: select()\n\nWhat if we want to keep every observation, but only use certain variables? Use select()!\n\nWe can select columns by name:\n\nflights |&gt; \n16  select(year, month, day)\n\n\n16\n\nYou can use a - before a variable name or a vector of variables to drop them from the data (i.e. select(-c(year, month, day))).\n\n\n\n\n# A tibble: 336,776 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#subset-columns-select-1",
    "href": "workshops/data_wrangling_r.html#subset-columns-select-1",
    "title": "",
    "section": "Subset Columns: select()\n",
    "text": "Subset Columns: select()\n\nWhat if we want to keep every observation, but only use certain variables? Use select()!\nWe can select columns between variables (inclusive):\n\nflights |&gt; \n17  select(year:day)\n\n\n17\n\nAdd a ! before year and you’ll drop this group of variables from the data.\n\n\n\n\n# A tibble: 336,776 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#subset-columns-select-2",
    "href": "workshops/data_wrangling_r.html#subset-columns-select-2",
    "title": "",
    "section": "Subset Columns: select()\n",
    "text": "Subset Columns: select()\n\nWhat if we want to keep every observation, but only use certain variables? Use select()!\nWe can select columns based on a condition:\n\nflights |&gt; \n18  select(where(is.character))\n\n\n18\n\nThere are a number of helper functions you can use with select() including starts_with(), ends_with(), contains() and num_range(). Read more about these and more here.\n\n\n\n\n# A tibble: 336,776 × 4\n   carrier tailnum origin dest \n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;\n 1 UA      N14228  EWR    IAH  \n 2 UA      N24211  LGA    IAH  \n 3 AA      N619AA  JFK    MIA  \n 4 B6      N804JB  JFK    BQN  \n 5 DL      N668DN  LGA    ATL  \n 6 UA      N39463  EWR    ORD  \n 7 B6      N516JB  EWR    FLL  \n 8 EV      N829AS  LGA    IAD  \n 9 B6      N593JB  JFK    MCO  \n10 AA      N3ALAA  LGA    ORD  \n# ℹ 336,766 more rows"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#finding-unique-rows-distinct",
    "href": "workshops/data_wrangling_r.html#finding-unique-rows-distinct",
    "title": "",
    "section": "Finding Unique Rows: distinct()\n",
    "text": "Finding Unique Rows: distinct()\n\nYou may want to find the unique combinations of variables in a dataset. Use distinct()\n\n\nflights |&gt; \n19  distinct(origin, dest)\n\n\n19\n\nFind all unique origin and destination pairs.\n\n\n\n\n# A tibble: 224 × 2\n   origin dest \n   &lt;chr&gt;  &lt;chr&gt;\n 1 EWR    IAH  \n 2 LGA    IAH  \n 3 JFK    MIA  \n 4 JFK    BQN  \n 5 LGA    ATL  \n 6 EWR    ORD  \n 7 EWR    FLL  \n 8 LGA    IAD  \n 9 JFK    MCO  \n10 LGA    ORD  \n# ℹ 214 more rows"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#distinct-drops-variables",
    "href": "workshops/data_wrangling_r.html#distinct-drops-variables",
    "title": "",
    "section": "\ndistinct() drops variables!",
    "text": "distinct() drops variables!\nBy default, distinct() drops unused variables. If you don’t want to drop them, add the argument .keep_all = TRUE:\n\n\nflights |&gt; \n20  distinct(origin, dest, .keep_all = TRUE)\n\n\n20\n\nIt’s not a coincidence that all of these distinct flights are on January 1: distinct() will find the first occurrence of a unique row in the dataset and discard the rest. Use count() if you’re looking for the number of occurrences.\n\n\n\n\n# A tibble: 224 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 214 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#count-unique-rows-count",
    "href": "workshops/data_wrangling_r.html#count-unique-rows-count",
    "title": "",
    "section": "Count Unique Rows: count()\n",
    "text": "Count Unique Rows: count()\n\n\n\nflights |&gt;\n21  count(origin, dest, sort = TRUE)\n\n\n21\n\nsort = TRUE arranges them in descending order of number of occurrences.\n\n\n\n\n# A tibble: 224 × 3\n   origin dest      n\n   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n 1 JFK    LAX   11262\n 2 LGA    ATL   10263\n 3 LGA    ORD    8857\n 4 JFK    SFO    8204\n 5 LGA    CLT    6168\n 6 EWR    ORD    6100\n 7 JFK    BOS    5898\n 8 LGA    MIA    5781\n 9 JFK    MCO    5464\n10 EWR    BOS    5327\n# ℹ 214 more rows"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#sorting-data-by-rows-arrange",
    "href": "workshops/data_wrangling_r.html#sorting-data-by-rows-arrange",
    "title": "",
    "section": "Sorting Data by Rows: arrange()\n",
    "text": "Sorting Data by Rows: arrange()\n\nSometimes it’s useful to sort rows in your data, in ascending (low to high) or descending (high to low) order. We do that with arrange().\n\n\nflights |&gt; \n22  arrange(year, month, day, dep_time)\n\n\n22\n\nIf you provide more than one column name, each additional column will be used to break ties in the values of preceding columns.\n\n\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#sorting-data-by-rows-arrange-1",
    "href": "workshops/data_wrangling_r.html#sorting-data-by-rows-arrange-1",
    "title": "",
    "section": "Sorting Data by Rows: arrange()\n",
    "text": "Sorting Data by Rows: arrange()\n\nTo sort in descending order, using desc() within arrange()\n\n\nflights |&gt; \n  arrange(desc(dep_delay))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#rename-variables-rename",
    "href": "workshops/data_wrangling_r.html#rename-variables-rename",
    "title": "",
    "section": "Rename Variables: rename()\n",
    "text": "Rename Variables: rename()\n\nYou may receive data with unintuitive variable names. Change them using rename().\n\n\nflights |&gt; \n23  rename(tail_num = tailnum)\n\n\n23\n\nrename(new_name = old_name) is the format. You can use janitor::clean_names() if you want to automate this process for a lot of variables.\n\n\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tail_num &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n\n Variable Syntax\n\n\nI recommend against using spaces in a name! It makes things really hard sometimes!!"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#create-new-columns-mutate",
    "href": "workshops/data_wrangling_r.html#create-new-columns-mutate",
    "title": "",
    "section": "Create New Columns: mutate()\n",
    "text": "Create New Columns: mutate()\n\nYou can add new columns to a data frame using mutate().\n\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n24    .before = 1\n  )\n\n\n24\n\nBy default, mutate() adds new columns on the right hand side of your dataset, which makes it difficult to see if anything happened. You can use the .before argument to specify which numeric index (or variable name) to move the newly created variable to. .after is an alternative argument for this.\n\n\n\n\n# A tibble: 336,776 × 21\n    gain speed  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1    -9  370.  2013     1     1      517            515         2      830\n 2   -16  374.  2013     1     1      533            529         4      850\n 3   -31  408.  2013     1     1      542            540         2      923\n 4    17  517.  2013     1     1      544            545        -1     1004\n 5    19  394.  2013     1     1      554            600        -6      812\n 6   -16  288.  2013     1     1      554            558        -4      740\n 7   -24  404.  2013     1     1      555            600        -5      913\n 8    11  259.  2013     1     1      557            600        -3      709\n 9     5  405.  2013     1     1      557            600        -3      838\n10   -10  319.  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#specifying-variables-to-keep-mutate",
    "href": "workshops/data_wrangling_r.html#specifying-variables-to-keep-mutate",
    "title": "",
    "section": "Specifying Variables to Keep: mutate()\n",
    "text": "Specifying Variables to Keep: mutate()\n\nYou can specify which columns to keep with the .keep argument:\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours,\n25    .keep = \"used\"\n  )\n\n\n25\n\n“used” retains only the variables used to create the new variables, which is useful for checking your work. Other options include: “all,” “unused,” and “none.”\n\n\n\n\n# A tibble: 336,776 × 6\n   dep_delay arr_delay air_time  gain hours gain_per_hour\n       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n 1         2        11      227    -9 3.78          -2.38\n 2         4        20      227   -16 3.78          -4.23\n 3         2        33      160   -31 2.67         -11.6 \n 4        -1       -18      183    17 3.05           5.57\n 5        -6       -25      116    19 1.93           9.83\n 6        -4        12      150   -16 2.5           -6.4 \n 7        -5        19      158   -24 2.63          -9.11\n 8        -3       -14       53    11 0.883         12.5 \n 9        -3        -8      140     5 2.33           2.14\n10        -2         8      138   -10 2.3           -4.35\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#move-variables-around-relocate",
    "href": "workshops/data_wrangling_r.html#move-variables-around-relocate",
    "title": "",
    "section": "Move Variables Around: relocate()\n",
    "text": "Move Variables Around: relocate()\n\nYou might want to collect related variables together or move important variables to the front. Use relocate()!\n\nflights |&gt; \n26  relocate(time_hour, air_time)\n\n\n26\n\nBy default relocate() moves variables to the front but you can also specify where to put them using the .before and .after arguments, just like in mutate().\n\n\n\n\n# A tibble: 336,776 × 19\n   time_hour           air_time  year month   day dep_time sched_dep_time\n   &lt;dttm&gt;                 &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n 1 2013-01-01 05:00:00      227  2013     1     1      517            515\n 2 2013-01-01 05:00:00      227  2013     1     1      533            529\n 3 2013-01-01 05:00:00      160  2013     1     1      542            540\n 4 2013-01-01 05:00:00      183  2013     1     1      544            545\n 5 2013-01-01 06:00:00      116  2013     1     1      554            600\n 6 2013-01-01 05:00:00      150  2013     1     1      554            558\n 7 2013-01-01 06:00:00      158  2013     1     1      555            600\n 8 2013-01-01 06:00:00       53  2013     1     1      557            600\n 9 2013-01-01 06:00:00      140  2013     1     1      557            600\n10 2013-01-01 06:00:00      138  2013     1     1      558            600\n# ℹ 336,766 more rows\n# ℹ 12 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n#   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;,\n#   dest &lt;chr&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#grouping-data-group_by",
    "href": "workshops/data_wrangling_r.html#grouping-data-group_by",
    "title": "",
    "section": "Grouping Data: group_by()\n",
    "text": "Grouping Data: group_by()\n\nIf you want to analyze your data by specific groupings, use group_by():\n\nflights |&gt; \n27  group_by(month)\n\n\n27\n\ngroup_by() doesn’t change the data but you’ll notice that the output indicates that it is “grouped by” month (Groups: month [12]). This means subsequent operations will now work “by month”.\n\n\n\n\n# A tibble: 336,776 × 19\n# Groups:   month [12]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#summarizing-data-summarize",
    "href": "workshops/data_wrangling_r.html#summarizing-data-summarize",
    "title": "",
    "section": "Summarizing Data: summarize()\n",
    "text": "Summarizing Data: summarize()\n\nsummarize() calculates summaries of variables in your data:\n\nCount the number of rows\nCalculate the mean\nCalculate the sum\nFind the minimum or maximum value\n\n\nYou can use any function inside summarize() that aggregates multiple values into a single value (like sd(), mean(), or max())."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#summarize-example",
    "href": "workshops/data_wrangling_r.html#summarize-example",
    "title": "",
    "section": "\nsummarize() Example",
    "text": "summarize() Example\nLet’s see what this looks like in our flights dataset:\n\n\nflights |&gt; \n  summarize(\n28    avg_delay = mean(dep_delay)\n  )\n\n\n28\n\nThe NA produced here is a result of calling mean on dep_delay. Any summarizing function will return NA if any of the values are NA. We can set na.rm = TRUE to change this behavior.\n\n\n\n\n# A tibble: 1 × 1\n  avg_delay\n      &lt;dbl&gt;\n1        NA"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#summarize-example-1",
    "href": "workshops/data_wrangling_r.html#summarize-example-1",
    "title": "",
    "section": "\nsummarize() Example",
    "text": "summarize() Example\nLet’s see what this looks like in our flights dataset:\n\nflights |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE) \n  )\n\n# A tibble: 1 × 1\n  avg_delay\n      &lt;dbl&gt;\n1      12.6"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#summarizing-data-by-groups",
    "href": "workshops/data_wrangling_r.html#summarizing-data-by-groups",
    "title": "",
    "section": "Summarizing Data by Groups",
    "text": "Summarizing Data by Groups\nWhat if we want to summarize data by our groups? Use group_by() and summarize()\n\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE)\n  )\n\n# A tibble: 12 × 2\n   month delay\n   &lt;int&gt; &lt;dbl&gt;\n 1     1 10.0 \n 2     2 10.8 \n 3     3 13.2 \n 4     4 13.9 \n 5     5 13.0 \n 6     6 20.8 \n 7     7 21.7 \n 8     8 12.6 \n 9     9  6.72\n10    10  6.24\n11    11  5.44\n12    12 16.6 \n\n\n\n\nBecause we did group_by() with month, then used summarize(), we get one row per value of month!"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#summarizing-data-by-groups-1",
    "href": "workshops/data_wrangling_r.html#summarizing-data-by-groups-1",
    "title": "",
    "section": "Summarizing Data by Groups",
    "text": "Summarizing Data by Groups\nYou can create any number of summaries in a single call to summarize().\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n29    n = n()\n  )\n\n\n29\n\nn() returns the number of rows in each group.\n\n\n\n\n# A tibble: 12 × 3\n   month delay     n\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1     1 10.0  27004\n 2     2 10.8  24951\n 3     3 13.2  28834\n 4     4 13.9  28330\n 5     5 13.0  28796\n 6     6 20.8  28243\n 7     7 21.7  29425\n 8     8 12.6  29327\n 9     9  6.72 27574\n10    10  6.24 28889\n11    11  5.44 27268\n12    12 16.6  28135"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#grouping-by-multiple-variables",
    "href": "workshops/data_wrangling_r.html#grouping-by-multiple-variables",
    "title": "",
    "section": "Grouping by Multiple Variables \n",
    "text": "Grouping by Multiple Variables \n\n\ndaily &lt;- flights |&gt; \n  group_by(year, month, day)  \ndaily\n\n# A tibble: 336,776 × 19\n# Groups:   year, month, day [365]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n Summary & Grouping Behavior\n\n\nWhen you summarize a tibble grouped by more than one variable, each summary peels off the last group. You can change the default behavior by setting the .groups argument to a different value, e.g., “drop” to drop all grouping or “keep” to preserve the same groups. The default is “drop_last”."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#remove-grouping-ungroup",
    "href": "workshops/data_wrangling_r.html#remove-grouping-ungroup",
    "title": "",
    "section": "Remove Grouping: ungroup()\n",
    "text": "Remove Grouping: ungroup()\n\n\ndaily |&gt; \n  ungroup() \n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#new-alternative-for-grouping-.by",
    "href": "workshops/data_wrangling_r.html#new-alternative-for-grouping-.by",
    "title": "",
    "section": "New Alternative for Grouping: .by\n",
    "text": "New Alternative for Grouping: .by\n\n\nflights |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n30    .by = month\n  )\n\n\n30\n\n.by works with all verbs and has the advantage that you don’t need to use the .groups argument to suppress the grouping message or ungroup() when you’re done.\n\n\n\n\n# A tibble: 12 × 3\n   month delay     n\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1     1 10.0  27004\n 2    10  6.24 28889\n 3    11  5.44 27268\n 4    12 16.6  28135\n 5     2 10.8  24951\n 6     3 13.2  28834\n 7     4 13.9  28330\n 8     5 13.0  28796\n 9     6 20.8  28243\n10     7 21.7  29425\n11     8 12.6  29327\n12     9  6.72 27574"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#select-specific-rows-per-group-slice_",
    "href": "workshops/data_wrangling_r.html#select-specific-rows-per-group-slice_",
    "title": "",
    "section": "Select Specific Rows Per Group: slice_*\n",
    "text": "Select Specific Rows Per Group: slice_*\n\nThere are five handy functions that allow you extract specific rows within each group:\n\n\ndf |&gt; slice_head(n = 1) takes the first row from each group.\n\ndf |&gt; slice_tail(n = 1) takes the last row in each group.\n\ndf |&gt; slice_min(x, n = 1) takes the row with the smallest value of column x.\n\ndf |&gt; slice_max(x, n = 1) takes the row with the largest value of column x.\n\ndf |&gt; slice_sample(n = 1) takes one random row.\n\n\nLet’s find the flights that are most delayed upon arrival at each destination."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#select-specific-rows-per-group-slice_-1",
    "href": "workshops/data_wrangling_r.html#select-specific-rows-per-group-slice_-1",
    "title": "",
    "section": "Select Specific Rows Per Group: slice_*\n",
    "text": "Select Specific Rows Per Group: slice_*\n\n\nflights |&gt; \n  group_by(dest) |&gt; \n31  slice_max(arr_delay, n = 1) |&gt;\n  relocate(dest) \n\n\n31\n\nYou can vary n to select more than one row, or instead of n =, you can use prop = 0.1 to select (e.g.) 10% of the rows in each group.\n\n\n\n\n# A tibble: 108 × 19\n# Groups:   dest [105]\n   dest   year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1 ABQ    2013     7    22     2145           2007        98      132\n 2 ACK    2013     7    23     1139            800       219     1250\n 3 ALB    2013     1    25      123           2000       323      229\n 4 ANC    2013     8    17     1740           1625        75     2042\n 5 ATL    2013     7    22     2257            759       898      121\n 6 AUS    2013     7    10     2056           1505       351     2347\n 7 AVL    2013     8    13     1156            832       204     1417\n 8 BDL    2013     2    21     1728           1316       252     1839\n 9 BGR    2013    12     1     1504           1056       248     1628\n10 BHM    2013     4    10       25           1900       325      136\n# ℹ 98 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\nThere are 105 groups but 108 rows! Why? slice_min() and slice_max() keep tied values so n = 1 means “give us all rows with the highest value.” If you want exactly one row per group you can set with_ties = FALSE."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#why-merge-data",
    "href": "workshops/data_wrangling_r.html#why-merge-data",
    "title": "",
    "section": "Why Merge Data?",
    "text": "Why Merge Data?\nIn practice, we often collect data from different sources. To analyze the data, we usually must first combine (merge) them.\n\nFor example, imagine you would like to study county-level patterns with respect to age and grocery spending. However, you can only find,\n\nCounty level age data from the US Census, and\nCounty level grocery spending data from the US Department of Agriculture\n\n\n\nMerge the data!!\n\n\nTo do this we’ll be using the various join functions from the dplyr package."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#joining-in-concept",
    "href": "workshops/data_wrangling_r.html#joining-in-concept",
    "title": "",
    "section": "Joining in Concept",
    "text": "Joining in Concept\nWe need to think about the following when we want to merge data frames A and B:\n\n\nWhich rows are we keeping from each data frame?\n\n\n\n\nWhich columns are we keeping from each data frame?\n\n\n\n\n\nWhich variables determine whether rows match?"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#keys",
    "href": "workshops/data_wrangling_r.html#keys",
    "title": "",
    "section": "Keys",
    "text": "Keys\nKeys are the way that two datasets are connected to one another. The two types of keys are:\n\n\nPrimary: a variable or set of variables that uniquely identifies each observation.\n\nWhen more than one variable makes up the primary key it’s called a compound key\n\n\n\n\nForeign: a variable (or set of variables) that corresponds to a primary key in another table."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#primary-keys",
    "href": "workshops/data_wrangling_r.html#primary-keys",
    "title": "",
    "section": "Primary Keys \n",
    "text": "Primary Keys \n\nLet’s look at our data to gain a better sense of what this all means.\n\n\nairlines\nairports\nplanes\nweather\nflights\n\n\n\n\nairlines records two pieces of data about each airline: its carrier code and its full name. You can identify an airline with its two letter carrier code, making carrier the primary key.\n\n\nairlines \n\n# A tibble: 16 × 2\n   carrier name                       \n   &lt;chr&gt;   &lt;chr&gt;                      \n 1 9E      Endeavor Air Inc.          \n 2 AA      American Airlines Inc.     \n 3 AS      Alaska Airlines Inc.       \n 4 B6      JetBlue Airways            \n 5 DL      Delta Air Lines Inc.       \n 6 EV      ExpressJet Airlines Inc.   \n 7 F9      Frontier Airlines Inc.     \n 8 FL      AirTran Airways Corporation\n 9 HA      Hawaiian Airlines Inc.     \n10 MQ      Envoy Air                  \n11 OO      SkyWest Airlines Inc.      \n12 UA      United Air Lines Inc.      \n13 US      US Airways Inc.            \n14 VX      Virgin America             \n15 WN      Southwest Airlines Co.     \n16 YV      Mesa Airlines Inc.         \n\n\n\n\n\nairports records data about each airport. You can identify each airport by its three letter airport code, making faa the primary key.\n\n\nairports\n\n# A tibble: 1,458 × 8\n   faa   name                             lat    lon   alt    tz dst   tzone    \n   &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    \n 1 04G   Lansdowne Airport               41.1  -80.6  1044    -5 A     America/…\n 2 06A   Moton Field Municipal Airport   32.5  -85.7   264    -6 A     America/…\n 3 06C   Schaumburg Regional             42.0  -88.1   801    -6 A     America/…\n 4 06N   Randall Airport                 41.4  -74.4   523    -5 A     America/…\n 5 09J   Jekyll Island Airport           31.1  -81.4    11    -5 A     America/…\n 6 0A9   Elizabethton Municipal Airport  36.4  -82.2  1593    -5 A     America/…\n 7 0G6   Williams County Airport         41.5  -84.5   730    -5 A     America/…\n 8 0G7   Finger Lakes Regional Airport   42.9  -76.8   492    -5 A     America/…\n 9 0P2   Shoestring Aviation Airfield    39.8  -76.6  1000    -5 U     America/…\n10 0S9   Jefferson County Intl           48.1 -123.    108    -8 A     America/…\n# ℹ 1,448 more rows\n\n\n\n\n\nplanes records data about each plane. You can identify a plane by its tail number, making tailnum the primary key.\n\n\nplanes\n\n# A tibble: 3,322 × 9\n   tailnum  year type              manufacturer model engines seats speed engine\n   &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; \n 1 N10156   2004 Fixed wing multi… EMBRAER      EMB-…       2    55    NA Turbo…\n 2 N102UW   1998 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 3 N103US   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 4 N104UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 5 N10575   2002 Fixed wing multi… EMBRAER      EMB-…       2    55    NA Turbo…\n 6 N105UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 7 N107US   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 8 N108UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 9 N109UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n10 N110UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n# ℹ 3,312 more rows\n\n\n\n\n\nweather records data about the weather at the origin airports. You can identify each observation by the combination of location and time, making origin and time_hour the compound primary key.\n\n\nweather\n\n# A tibble: 26,115 × 15\n   origin  year month   day  hour  temp  dewp humid wind_dir wind_speed\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4 \n 2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06\n 3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5 \n 4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7 \n 5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7 \n 6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5 \n 7 EWR     2013     1     1     7  39.0  28.0  64.4      240      15.0 \n 8 EWR     2013     1     1     8  39.9  28.0  62.2      250      10.4 \n 9 EWR     2013     1     1     9  39.9  28.0  62.2      260      15.0 \n10 EWR     2013     1     1    10  41    28.0  59.6      260      13.8 \n# ℹ 26,105 more rows\n# ℹ 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;,\n#   visib &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nflights has three variables (time_hour, flight, carrier) that uniquely identify an observation. More significantly, however, it contains foreign keys that correspond to the primary keys of the other datasets.\n\n\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#foreign-keys",
    "href": "workshops/data_wrangling_r.html#foreign-keys",
    "title": "",
    "section": "Foreign Keys",
    "text": "Foreign Keys\n\nNote: grey shading indicates the primary key for that particular dataset.\n\nflights$origin –&gt; airports$faa\n\n\nflights$dest –&gt; airports$faa\n\n\nflights$origin-flights$time_hour –&gt; weather$origin-weather$time_hour.\n\nflights$tailnum –&gt; planes$tailnum\n\n\nflights$carrier –&gt; airlines$carrier"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#checking-keys",
    "href": "workshops/data_wrangling_r.html#checking-keys",
    "title": "",
    "section": "Checking Keys",
    "text": "Checking Keys\nA nice feature of these data are that the primary and foreign keys have the same name and almost every variable name used across multiple tables has the same meaning.1 This isn’t always the case!2\n\nIt is good practice to make sure your primary keys actually uniquely identify an observation and that they don’t have any missing values.\n\n\n\nplanes |&gt; \n1  count(tailnum) |&gt;\n  filter(n &gt; 1)\n\n\n1\n\nIf your primary keys uniquely identify each observation you’ll get an empty tibble in return.\n\n\n\n\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt;\n\n\n\n\n\nplanes |&gt; \n2  filter(is.na(tailnum))\n\n\n2\n\nIf none of your primary keys are missing you’ll get an empty tibble in return here too.\n\n\n\n\n# A tibble: 0 × 9\n# ℹ 9 variables: tailnum &lt;chr&gt;, year &lt;int&gt;, type &lt;chr&gt;, manufacturer &lt;chr&gt;,\n#   model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\n\n\nWith the exception of year: it means year of departure in flights and year of manufacture in planes. We’ll cover how to handle this shortly."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#surrogate-keys",
    "href": "workshops/data_wrangling_r.html#surrogate-keys",
    "title": "",
    "section": "Surrogate Keys",
    "text": "Surrogate Keys\nSometimes you’ll want to create an index of your observations to serve as a surrogate key because the compound primary key is not particlarly easy to reference.\n\nFor example, our flights dataset has three variables that uniquely identify each observation: time_hour, carrier, flight.\n\n\n\nflights2 &lt;- flights |&gt; \n3  mutate(id = row_number(), .before = 1)\nflights2\n\n\n3\n\nrow_number() simply specifies the row number of the dataframe.\n\n\n\n\n\n\n# A tibble: 336,776 × 20\n      id  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1     1  2013     1     1      517            515         2      830\n 2     2  2013     1     1      533            529         4      850\n 3     3  2013     1     1      542            540         2      923\n 4     4  2013     1     1      544            545        -1     1004\n 5     5  2013     1     1      554            600        -6      812\n 6     6  2013     1     1      554            558        -4      740\n 7     7  2013     1     1      555            600        -5      913\n 8     8  2013     1     1      557            600        -3      709\n 9     9  2013     1     1      557            600        -3      838\n10    10  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#basic-equi--joins",
    "href": "workshops/data_wrangling_r.html#basic-equi--joins",
    "title": "",
    "section": "Basic (Equi-) Joins",
    "text": "Basic (Equi-) Joins\nAll join functions have the same basic interface: they take a pair of data frames and return one data frame.\n\nThe order of the rows and columns is primarily going to be determined by the first data frame.\n\n\ndplyr has two types of joins: mutating and filtering.\n\n\n\n\nMutating Joins\nAdd new variables to one data frame from matching observations from another data frame.\n\nleft_join()\nright_join()\ninner_join()\nfull_join()\n\n\n\n\nFiltering Joins\nFilter observations from one data frame based on whether or not they match an observation in another data frame.\n\nsemi_join()\nanti-join()"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#mutating-joins-1",
    "href": "workshops/data_wrangling_r.html#mutating-joins-1",
    "title": "",
    "section": "Mutating Joins",
    "text": "Mutating Joins"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#left_join",
    "href": "workshops/data_wrangling_r.html#left_join",
    "title": "",
    "section": "\nleft_join() \n",
    "text": "left_join() \n\n\n\n\n\n\n\n\nThe most common type of join\nAppends columns from y to x by the rows in x\n\n\nNA added if there is nothing from y\n\n\n\nNatural join: when all variables that appear in both datasets are used as the join key\n\nIf the join_by() argument is not specified, left_join() will automatically join by all columns that have names and values in common."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#left_join-in-nycflights13",
    "href": "workshops/data_wrangling_r.html#left_join-in-nycflights13",
    "title": "",
    "section": "\nleft_join in nycflights13\n",
    "text": "left_join in nycflights13\n\n\nflights2 &lt;- flights |&gt; \n  select(year, time_hour, origin, dest, tailnum, carrier)\n\nWith only the pertinent variables from the flights dataset, we can see how a left_join works with the airlines dataset.\n\nflights2 |&gt;\n  left_join(airlines)\n\n\n\nJoining with `by = join_by(carrier)`\n\n\n# A tibble: 336,776 × 7\n    year time_hour           origin dest  tailnum carrier name                  \n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                 \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      United Air Lines Inc. \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      United Air Lines Inc. \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      American Airlines Inc.\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      JetBlue Airways       \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Delta Air Lines Inc.  \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      United Air Lines Inc. \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      JetBlue Airways       \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      ExpressJet Airlines I…\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      JetBlue Airways       \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      American Airlines Inc.\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#different-variable-meanings",
    "href": "workshops/data_wrangling_r.html#different-variable-meanings",
    "title": "",
    "section": "Different variable meanings",
    "text": "Different variable meanings\n\nflights2 |&gt; \n  left_join(planes)\n\n\n\nJoining with `by = join_by(year, tailnum)`\n\n\n# A tibble: 336,776 × 13\n    year time_hour           origin dest  tailnum carrier type  manufacturer\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      &lt;NA&gt;  &lt;NA&gt;        \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      &lt;NA&gt;  &lt;NA&gt;        \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      &lt;NA&gt;  &lt;NA&gt;        \n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;  &lt;NA&gt;        \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      &lt;NA&gt;  &lt;NA&gt;        \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      &lt;NA&gt;  &lt;NA&gt;        \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      &lt;NA&gt;  &lt;NA&gt;        \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      &lt;NA&gt;  &lt;NA&gt;        \n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      &lt;NA&gt;  &lt;NA&gt;        \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;  &lt;NA&gt;        \n# ℹ 336,766 more rows\n# ℹ 5 more variables: model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;,\n#   engine &lt;chr&gt;\n\n\n\nWhen we try to do this, however, we get a bunch of NAs. Why?"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#different-variable-meanings-1",
    "href": "workshops/data_wrangling_r.html#different-variable-meanings-1",
    "title": "",
    "section": "Different variable meanings",
    "text": "Different variable meanings\n\nflights2 |&gt; \n  left_join(planes)\n\nJoining with `by = join_by(year, tailnum)`\n\n\n# A tibble: 336,776 × 13\n    year time_hour           origin dest  tailnum carrier type  manufacturer\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      &lt;NA&gt;  &lt;NA&gt;        \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      &lt;NA&gt;  &lt;NA&gt;        \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      &lt;NA&gt;  &lt;NA&gt;        \n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;  &lt;NA&gt;        \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      &lt;NA&gt;  &lt;NA&gt;        \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      &lt;NA&gt;  &lt;NA&gt;        \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      &lt;NA&gt;  &lt;NA&gt;        \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      &lt;NA&gt;  &lt;NA&gt;        \n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      &lt;NA&gt;  &lt;NA&gt;        \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;  &lt;NA&gt;        \n# ℹ 336,766 more rows\n# ℹ 5 more variables: model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;,\n#   engine &lt;chr&gt;\n\n\nJoin is trying to use tailnum and year as a compound key. While both datasets have year as a variable, they mean different things. Therefore, we need to be explicit here about what to join by."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#different-variable-meanings-2",
    "href": "workshops/data_wrangling_r.html#different-variable-meanings-2",
    "title": "",
    "section": "Different variable meanings",
    "text": "Different variable meanings\n\nflights2 |&gt; \n4  left_join(planes, join_by(tailnum))\n\n\n4\n\njoin_by(tailnum) is short for join_by(tailnum == tailnum) making these types of basic joins equi joins.\n\n\n\n\n\n\n# A tibble: 336,776 × 14\n   year.x time_hour           origin dest  tailnum carrier year.y type          \n    &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;         \n 1   2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA        1999 Fixed wing mu…\n 2   2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA        1998 Fixed wing mu…\n 3   2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA        1990 Fixed wing mu…\n 4   2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6        2012 Fixed wing mu…\n 5   2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL        1991 Fixed wing mu…\n 6   2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA        2012 Fixed wing mu…\n 7   2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6        2000 Fixed wing mu…\n 8   2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV        1998 Fixed wing mu…\n 9   2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6        2004 Fixed wing mu…\n10   2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA          NA &lt;NA&gt;          \n# ℹ 336,766 more rows\n# ℹ 6 more variables: manufacturer &lt;chr&gt;, model &lt;chr&gt;, engines &lt;int&gt;,\n#   seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\n\n\n\nWhen you have the same variable name but they mean different things you can specify a particular suffix with the suffix argument."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#different-variable-names",
    "href": "workshops/data_wrangling_r.html#different-variable-names",
    "title": "",
    "section": "Different variable names",
    "text": "Different variable names\nIf you have keys that have the same meaning (values) but are named different things in their respective datasets you’d also specify that with join_by()\n\n\nflights2 |&gt; \n5  left_join(airports, join_by(dest == faa))\n\n\n5\n\nby = c(\"dest\" = \"faa\") was the former syntax for this and you still might see that in older code.\n\n\n\n\n\n\n# A tibble: 336,776 × 13\n    year time_hour           origin dest  tailnum carrier name         lat   lon\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      George Bu…  30.0 -95.3\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      George Bu…  30.0 -95.3\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Miami Intl  25.8 -80.3\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;        NA    NA  \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Hartsfiel…  33.6 -84.4\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Chicago O…  42.0 -87.9\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      Fort Laud…  26.1 -80.2\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      Washingto…  38.9 -77.5\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      Orlando I…  28.4 -81.3\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      Chicago O…  42.0 -87.9\n# ℹ 336,766 more rows\n# ℹ 4 more variables: alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n\n\n\nThis will match dest to faa for the join and then drop faa."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#different-variable-names-1",
    "href": "workshops/data_wrangling_r.html#different-variable-names-1",
    "title": "",
    "section": "Different variable names",
    "text": "Different variable names\nYou can request dplyr to keep both keys with keep = TRUE argument.\n\n\nflights2 |&gt; \n  left_join(airports, join_by(dest == faa), keep = TRUE) \n\n\n\n# A tibble: 336,776 × 14\n    year time_hour           origin dest  tailnum carrier faa   name         lat\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      IAH   George Bu…  30.0\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      IAH   George Bu…  30.0\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      MIA   Miami Intl  25.8\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;  &lt;NA&gt;        NA  \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      ATL   Hartsfiel…  33.6\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      ORD   Chicago O…  42.0\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      FLL   Fort Laud…  26.1\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      IAD   Washingto…  38.9\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      MCO   Orlando I…  28.4\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      ORD   Chicago O…  42.0\n# ℹ 336,766 more rows\n# ℹ 5 more variables: lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#right_join",
    "href": "workshops/data_wrangling_r.html#right_join",
    "title": "",
    "section": "right_join()",
    "text": "right_join()\n\nHas the same interface as a left_join but keeps all rows in y instead of x"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#inner_join",
    "href": "workshops/data_wrangling_r.html#inner_join",
    "title": "",
    "section": "inner_join()",
    "text": "inner_join()\n\nHas the same interface as a left_join but only keeps rows that occur in both x and y"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#full_join",
    "href": "workshops/data_wrangling_r.html#full_join",
    "title": "",
    "section": "full_join()",
    "text": "full_join()\n\nHas the same interface as a left_join but keeps all rows in either x or y"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#filtering-joins-1",
    "href": "workshops/data_wrangling_r.html#filtering-joins-1",
    "title": "",
    "section": "Filtering Joins",
    "text": "Filtering Joins"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#semi_join",
    "href": "workshops/data_wrangling_r.html#semi_join",
    "title": "",
    "section": "semi_join()",
    "text": "semi_join()\n\nKeeps all rows in x that have a match in y"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#semi_join-in-nycflights13",
    "href": "workshops/data_wrangling_r.html#semi_join-in-nycflights13",
    "title": "",
    "section": "\nsemi_join() in nycflights13\n",
    "text": "semi_join() in nycflights13\n\nWe could use a semi-join to filter the airports dataset to show just the origin airports.\n\n\nairports |&gt; \n  semi_join(flights2, join_by(faa == origin))\n\n\n\n# A tibble: 3 × 8\n  faa   name                  lat   lon   alt    tz dst   tzone           \n  &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           \n1 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n2 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n3 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#anti_join",
    "href": "workshops/data_wrangling_r.html#anti_join",
    "title": "",
    "section": "anti_join()",
    "text": "anti_join()\n\nReturns all rows in x that don’t have a match in y"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#anti_join-in-nycflights13",
    "href": "workshops/data_wrangling_r.html#anti_join-in-nycflights13",
    "title": "",
    "section": "\nanti_join() in nycflights13\n",
    "text": "anti_join() in nycflights13\n\nWe can find rows that are missing from airports by looking for flights that don’t have a matching destination airport.\n\n\nairports |&gt; \n  anti_join(flights2, join_by(faa == origin))\n\n\n\n# A tibble: 1,455 × 8\n   faa   name                             lat    lon   alt    tz dst   tzone    \n   &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    \n 1 04G   Lansdowne Airport               41.1  -80.6  1044    -5 A     America/…\n 2 06A   Moton Field Municipal Airport   32.5  -85.7   264    -6 A     America/…\n 3 06C   Schaumburg Regional             42.0  -88.1   801    -6 A     America/…\n 4 06N   Randall Airport                 41.4  -74.4   523    -5 A     America/…\n 5 09J   Jekyll Island Airport           31.1  -81.4    11    -5 A     America/…\n 6 0A9   Elizabethton Municipal Airport  36.4  -82.2  1593    -5 A     America/…\n 7 0G6   Williams County Airport         41.5  -84.5   730    -5 A     America/…\n 8 0G7   Finger Lakes Regional Airport   42.9  -76.8   492    -5 A     America/…\n 9 0P2   Shoestring Aviation Airfield    39.8  -76.6  1000    -5 U     America/…\n10 0S9   Jefferson County Intl           48.1 -123.    108    -8 A     America/…\n# ℹ 1,445 more rows\n\n\n\n\n\nThis type of join is useful for finding missing values that are implicit in the data (i.e. NAs that don’t show up in the data but only exist as an absence.)"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#more-than-one-match",
    "href": "workshops/data_wrangling_r.html#more-than-one-match",
    "title": "",
    "section": "More Than One Match",
    "text": "More Than One Match\n\n\nThere are three possible outcomes for a row in x:\n\nIf it doesn’t match anything, it’s dropped.\nIf it matches 1 row in y, it’s preserved.\nIf it matches more than 1 row in y, it’s duplicated once for each match.\n\n\n\nWhat happens if we match on more than one row?"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#more-than-one-match-1",
    "href": "workshops/data_wrangling_r.html#more-than-one-match-1",
    "title": "",
    "section": "More Than One Match",
    "text": "More Than One Match\n\ndf1 &lt;- tibble(key = c(1, 2, 2), val_x = c(\"x1\", \"x2\", \"x3\"))\ndf2 &lt;- tibble(key = c(1, 2, 2), val_y = c(\"y1\", \"y2\", \"y3\"))\n\ndf1 |&gt; \n  inner_join(df2, join_by(key))\n\n\n\nWarning in inner_join(df1, df2, join_by(key)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 2 of `x` matches multiple rows in `y`.\nℹ Row 2 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 5 × 3\n    key val_x val_y\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x1    y1   \n2     2 x2    y2   \n3     2 x2    y3   \n4     2 x3    y2   \n5     2 x3    y3   \n\n\n\nIf you’re doing this deliberately, set relationship = “many-to-many”, as the warning suggests.\n\n\n\nGiven their nature, filtering joins never duplicate rows like mutating joins do. They will only ever return a subset of the datasets."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#non-equi-joins",
    "href": "workshops/data_wrangling_r.html#non-equi-joins",
    "title": "",
    "section": "Non-Equi Joins",
    "text": "Non-Equi Joins\nThe joins we’ve discussed thus far have all been equi-joins, where the rows match if the x key equals the y key. But you can also specify other types of relationships.\n\ndplyr has four different types of non-equi joins:\n\n\n\n\n\n\nCross joins match every pair of rows.\n\n\n\n\n\n\n\nCross joins, aka self-joins, are useful when generating permutations (e.g. creating every possible combination of values). This comes in handy when creating datasets of predicted probabilities for plotting in ggplot."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#non-equi-joins-1",
    "href": "workshops/data_wrangling_r.html#non-equi-joins-1",
    "title": "",
    "section": "Non-Equi Joins",
    "text": "Non-Equi Joins\nThe joins we’ve discussed thus far have all been equi-joins, where the rows match if the x key equals the y key. But you can also specify other types of relationships.\ndplyr has four different types of non-equi joins:\n\n\n\n\nCross joins match every pair of rows.\n\nInequality joins use &lt;, &lt;=, &gt;, and &gt;= instead of ==.\n\n\nOverlap joins are a special type of inequality join designed to work with ranges1.\n\n\n\n\n\n\n\n\nInequality joins can be used to restrict the cross join so that instead of generating all permutations, we generate all combinations.\nOverlap joins provide three helpers that use inequality joins to make it easier to work with intervals: between(), within(), overlaps(). Read more about their functionality and specifications here."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#non-equi-joins-2",
    "href": "workshops/data_wrangling_r.html#non-equi-joins-2",
    "title": "",
    "section": "Non-Equi Joins",
    "text": "Non-Equi Joins\nThe joins we’ve discussed thus far have all been equi-joins, where the rows match if the x key equals the y key. But you can also specify other types of relationships.\ndplyr has four different types of non-equi joins:\n\n\n\n\nCross joins match every pair of rows.\n\nInequality joins use &lt;, &lt;=, &gt;, and &gt;= instead of ==.\n\n\nOverlap joins are a special type of inequality join designed to work with ranges.\n\n\n\nRolling joins are similar to inequality joins but only find the closest match.\n\n\n\n\n\n\nRolling joins are a special type of inequality join where instead of getting every row that satisfies the inequality, you get just the closest row. You can turn any inequality join into a rolling join by adding closest()."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#what-is-tidy-data",
    "href": "workshops/data_wrangling_r.html#what-is-tidy-data",
    "title": "",
    "section": "What is Tidy Data",
    "text": "What is Tidy Data\nTidy data1 (aka “long data”) are such that:\n\n\n\n\n\nThe values for a single variable are in their own column.\nThe values for a single observation are in their own row.\nThere is only one value per cell.\nRead the original article here."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#why-do-we-want-tidy-data",
    "href": "workshops/data_wrangling_r.html#why-do-we-want-tidy-data",
    "title": "",
    "section": "Why do we Want Tidy Data?",
    "text": "Why do we Want Tidy Data?\n\n\nEasier to understand many rows than many columns1\n\nRequired for plotting in ggplot22\n\nRequired for many types of statistical procedures (e.g. hierarchical or mixed effects models)\nFewer issues with missing values and “imbalanced” repeated measures data\nHaving a consistent method for storing data means it’s easier to learn the tools to work with it since there’s an underlying uniformity.\n\n\nMost real-world data is not tidy because data are often organized for goals other than analysis (i.e. data entry) and most people aren’t familiar with the principles of tidy data.\n\nPlacing variables in columns also leverages R’s vectorized nature, i.e. most built-in R functions work with values of vectors.In fact, all tidyverse functions are designed to work with tidy data."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#slightly-messy-data",
    "href": "workshops/data_wrangling_r.html#slightly-messy-data",
    "title": "",
    "section": "Slightly “Messy” Data",
    "text": "Slightly “Messy” Data\n\n\n\n\nProgram\nFirst Year\nSecond Year\n\n\n\nEvans School\n10\n6\n\n\nArts & Sciences\n5\n6\n\n\nPublic Health\n2\n3\n\n\nOther\n5\n1\n\n\n\n\n\nWhat is an observation?\n\nA group of students from a program of a given year\n\n\nWhat are the variables?\n\nProgram, Year\n\n\nWhat are the values?\n\nProgram: Evans School, Arts & Sciences, Public Health, Other\nYear: First, Second – in column headings. Bad!\n\nCount: spread over two columns!"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#tidy-version",
    "href": "workshops/data_wrangling_r.html#tidy-version",
    "title": "",
    "section": "Tidy Version",
    "text": "Tidy Version\n\n\n\n\nProgram\nYear\nCount\n\n\n\nEvans School\nFirst\n10\n\n\nEvans School\nSecond\n6\n\n\nArts & Sciences\nFirst\n5\n\n\nArts & Sciences\nSecond\n6\n\n\nPublic Health\nFirst\n2\n\n\nPublic Health\nSecond\n3\n\n\nOther\nFirst\n5\n\n\nOther\nSecond\n1\n\n\n\n\n\nEach variable is a column.\nEach observation is a row.\nEach cell has a single value."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#billboard-data",
    "href": "workshops/data_wrangling_r.html#billboard-data",
    "title": "",
    "section": "Billboard Data",
    "text": "Billboard Data\n\n\nRows: 317\nColumns: 81\n$ year         &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200…\n$ artist       &lt;chr&gt; \"2 Pac\", \"2Ge+her\", \"3 Doors Down\", \"3 Doors Down\", \"504 …\n$ track        &lt;chr&gt; \"Baby Don't Cry (Keep...\", \"The Hardest Part Of ...\", \"Kr…\n$ time         &lt;time&gt; 04:22:00, 03:15:00, 03:53:00, 04:24:00, 03:35:00, 03:24:…\n$ date.entered &lt;date&gt; 2000-02-26, 2000-09-02, 2000-04-08, 2000-10-21, 2000-04-…\n$ wk1          &lt;int&gt; 87, 91, 81, 76, 57, 51, 97, 84, 59, 76, 84, 57, 50, 71, 7…\n$ wk2          &lt;int&gt; 82, 87, 70, 76, 34, 39, 97, 62, 53, 76, 84, 47, 39, 51, 6…\n$ wk3          &lt;int&gt; 72, 92, 68, 72, 25, 34, 96, 51, 38, 74, 75, 45, 30, 28, 5…\n$ wk4          &lt;int&gt; 77, NA, 67, 69, 17, 26, 95, 41, 28, 69, 73, 29, 28, 18, 4…\n$ wk5          &lt;int&gt; 87, NA, 66, 67, 17, 26, 100, 38, 21, 68, 73, 23, 21, 13, …\n$ wk6          &lt;int&gt; 94, NA, 57, 65, 31, 19, NA, 35, 18, 67, 69, 18, 19, 13, 3…\n$ wk7          &lt;int&gt; 99, NA, 54, 55, 36, 2, NA, 35, 16, 61, 68, 11, 20, 11, 34…\n$ wk8          &lt;int&gt; NA, NA, 53, 59, 49, 2, NA, 38, 14, 58, 65, 9, 17, 1, 29, …\n$ wk9          &lt;int&gt; NA, NA, 51, 62, 53, 3, NA, 38, 12, 57, 73, 9, 17, 1, 27, …\n$ wk10         &lt;int&gt; NA, NA, 51, 61, 57, 6, NA, 36, 10, 59, 83, 11, 17, 2, 30,…\n$ wk11         &lt;int&gt; NA, NA, 51, 61, 64, 7, NA, 37, 9, 66, 92, 1, 17, 2, 36, N…\n$ wk12         &lt;int&gt; NA, NA, 51, 59, 70, 22, NA, 37, 8, 68, NA, 1, 3, 3, 37, N…\n$ wk13         &lt;int&gt; NA, NA, 47, 61, 75, 29, NA, 38, 6, 61, NA, 1, 3, 3, 39, N…\n$ wk14         &lt;int&gt; NA, NA, 44, 66, 76, 36, NA, 49, 1, 67, NA, 1, 7, 4, 49, N…\n$ wk15         &lt;int&gt; NA, NA, 38, 72, 78, 47, NA, 61, 2, 59, NA, 4, 10, 12, 57,…\n$ wk16         &lt;int&gt; NA, NA, 28, 76, 85, 67, NA, 63, 2, 63, NA, 8, 17, 11, 63,…\n$ wk17         &lt;int&gt; NA, NA, 22, 75, 92, 66, NA, 62, 2, 67, NA, 12, 25, 13, 65…\n$ wk18         &lt;int&gt; NA, NA, 18, 67, 96, 84, NA, 67, 2, 71, NA, 22, 29, 15, 68…\n$ wk19         &lt;int&gt; NA, NA, 18, 73, NA, 93, NA, 83, 3, 79, NA, 23, 29, 18, 79…\n$ wk20         &lt;int&gt; NA, NA, 14, 70, NA, 94, NA, 86, 4, 89, NA, 43, 40, 20, 86…\n$ wk21         &lt;int&gt; NA, NA, 12, NA, NA, NA, NA, NA, 5, NA, NA, 44, 43, 30, NA…\n$ wk22         &lt;int&gt; NA, NA, 7, NA, NA, NA, NA, NA, 5, NA, NA, NA, 50, 40, NA,…\n$ wk23         &lt;int&gt; NA, NA, 6, NA, NA, NA, NA, NA, 6, NA, NA, NA, NA, 39, NA,…\n$ wk24         &lt;int&gt; NA, NA, 6, NA, NA, NA, NA, NA, 9, NA, NA, NA, NA, 44, NA,…\n$ wk25         &lt;int&gt; NA, NA, 6, NA, NA, NA, NA, NA, 13, NA, NA, NA, NA, NA, NA…\n$ wk26         &lt;int&gt; NA, NA, 5, NA, NA, NA, NA, NA, 14, NA, NA, NA, NA, NA, NA…\n$ wk27         &lt;int&gt; NA, NA, 5, NA, NA, NA, NA, NA, 16, NA, NA, NA, NA, NA, NA…\n$ wk28         &lt;int&gt; NA, NA, 4, NA, NA, NA, NA, NA, 23, NA, NA, NA, NA, NA, NA…\n$ wk29         &lt;int&gt; NA, NA, 4, NA, NA, NA, NA, NA, 22, NA, NA, NA, NA, NA, NA…\n$ wk30         &lt;int&gt; NA, NA, 4, NA, NA, NA, NA, NA, 33, NA, NA, NA, NA, NA, NA…\n$ wk31         &lt;int&gt; NA, NA, 4, NA, NA, NA, NA, NA, 36, NA, NA, NA, NA, NA, NA…\n$ wk32         &lt;int&gt; NA, NA, 3, NA, NA, NA, NA, NA, 43, NA, NA, NA, NA, NA, NA…\n$ wk33         &lt;int&gt; NA, NA, 3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk34         &lt;int&gt; NA, NA, 3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk35         &lt;int&gt; NA, NA, 4, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk36         &lt;int&gt; NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk37         &lt;int&gt; NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk38         &lt;int&gt; NA, NA, 9, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk39         &lt;int&gt; NA, NA, 9, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk40         &lt;int&gt; NA, NA, 15, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk41         &lt;int&gt; NA, NA, 14, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk42         &lt;int&gt; NA, NA, 13, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk43         &lt;int&gt; NA, NA, 14, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk44         &lt;int&gt; NA, NA, 16, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk45         &lt;int&gt; NA, NA, 17, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk46         &lt;int&gt; NA, NA, 21, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk47         &lt;int&gt; NA, NA, 22, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk48         &lt;int&gt; NA, NA, 24, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk49         &lt;int&gt; NA, NA, 28, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk50         &lt;int&gt; NA, NA, 33, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk51         &lt;int&gt; NA, NA, 42, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk52         &lt;int&gt; NA, NA, 42, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk53         &lt;int&gt; NA, NA, 49, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk54         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk55         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk56         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk57         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk58         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk59         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk60         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk61         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk62         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk63         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk64         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk65         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk66         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk67         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk68         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk69         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk70         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk71         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk72         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk73         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk74         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk75         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk76         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\n\nThis data is just ugly-messy!\n\n\n\nWeek columns continue up to wk76!"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#how-is-billboard-currently-organized",
    "href": "workshops/data_wrangling_r.html#how-is-billboard-currently-organized",
    "title": "",
    "section": "How is Billboard currently organized?",
    "text": "How is Billboard currently organized?\n\nWhat are the observations in the data?\n\nSong on the Billboard chart each week\n\n\nWhat are the variables in the data?\n\nYear, artist, track, song length, date entered Hot 100, week since first entered Hot 100 (spread over many columns), rank during week (spread over many columns)\n\n\nWhat are the values in the data?\n\ne.g. 2000; 3 Doors Down; Kryptonite; 3 minutes 53 seconds; April 8, 2000; Week 3 (stuck in column headings); rank 68 (spread over many columns)"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#tidyr",
    "href": "workshops/data_wrangling_r.html#tidyr",
    "title": "",
    "section": "tidyr",
    "text": "tidyr\nThe tidyr package provides functions to tidy up data.\n\nKey functions:\n\n\npivot_longer(): takes a set of columns and pivots them down (“longer”) to make two new columns (which you can name yourself):\n\nA name column that stores the original column names\nA value with the values in those original columns\n\n\n\n\n\n\n\npivot_wider(): inverts pivot_longer() by taking two columns and pivoting them up and across (“wider”) into multiple columns"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#pivot_longer",
    "href": "workshops/data_wrangling_r.html#pivot_longer",
    "title": "",
    "section": "pivot_longer()",
    "text": "pivot_longer()\nThis function usually takes three arguments:\n\n\ncols: The columns that need to be pivoted (are not variables)\n\nnames_to: Names the new variable that is stored in multiple columns\n\nvalues_to: Names the variable stored in the cell values"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#pivot_longer-1",
    "href": "workshops/data_wrangling_r.html#pivot_longer-1",
    "title": "",
    "section": "pivot_longer()",
    "text": "pivot_longer()\nThis function usually takes three arguments:\n\n\ncols: The columns that need to be pivoted (are not variables)\nnames_to: Names the new variable that is stored in multiple columns\n\nvalues_to: Names the variable stored in the cell values"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#pivot_longer-2",
    "href": "workshops/data_wrangling_r.html#pivot_longer-2",
    "title": "",
    "section": "pivot_longer()",
    "text": "pivot_longer()\nThis function usually takes three arguments:\n\n\ncols: The columns that need to be pivoted (are not variables)\n\nnames_to: Names the new variable that is stored in multiple columns\nvalues_to: Names the variable stored in the cell values"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#pivot_longer-3",
    "href": "workshops/data_wrangling_r.html#pivot_longer-3",
    "title": "",
    "section": "pivot_longer()",
    "text": "pivot_longer()\nThis function usually takes three arguments:\n\n\ncols: The columns that need to be pivoted (are not variables)\n\nnames_to: Names the new variable that is stored in multiple columns\n\nvalues_to: Names the variable stored in the cell values"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#pivot_longer-example",
    "href": "workshops/data_wrangling_r.html#pivot_longer-example",
    "title": "",
    "section": "\npivot_longer() Example",
    "text": "pivot_longer() Example\n\nbillboard_2000 &lt;- billboard_2000_raw |&gt; \n1  pivot_longer(cols = starts_with(\"wk\"),\n               names_to =\"week\",\n               values_to = \"rank\")\n\nbillboard_2000 |&gt; head(10)\n\n\n1\n\nstarts_with() is one of the helper functions from tidyselect that helps select certain common patterns. We could have also used cols = wk1:wk76.\n\n\n\n\n\n\n# A tibble: 10 × 7\n    year artist track                   time   date.entered week   rank\n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;                   &lt;time&gt; &lt;date&gt;       &lt;chr&gt; &lt;int&gt;\n 1  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26   wk1      87\n 2  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26   wk2      82\n 3  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26   wk3      72\n 4  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26   wk4      77\n 5  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26   wk5      87\n 6  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26   wk6      94\n 7  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26   wk7      99\n 8  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26   wk8      NA\n 9  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26   wk9      NA\n10  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26   wk10     NA\n\n\n\nNow we have a single week column!"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#lots-of-missing-values",
    "href": "workshops/data_wrangling_r.html#lots-of-missing-values",
    "title": "",
    "section": "Lots of Missing Values?!",
    "text": "Lots of Missing Values?!\n\nglimpse(billboard_2000)\n\n\n\nRows: 24,092\nColumns: 7\n$ year         &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200…\n$ artist       &lt;chr&gt; \"2 Pac\", \"2 Pac\", \"2 Pac\", \"2 Pac\", \"2 Pac\", \"2 Pac\", \"2 …\n$ track        &lt;chr&gt; \"Baby Don't Cry (Keep...\", \"Baby Don't Cry (Keep...\", \"Ba…\n$ time         &lt;time&gt; 04:22:00, 04:22:00, 04:22:00, 04:22:00, 04:22:00, 04:22:…\n$ date.entered &lt;date&gt; 2000-02-26, 2000-02-26, 2000-02-26, 2000-02-26, 2000-02-…\n$ week         &lt;chr&gt; \"wk1\", \"wk2\", \"wk3\", \"wk4\", \"wk5\", \"wk6\", \"wk7\", \"wk8\", \"…\n$ rank         &lt;int&gt; 87, 82, 72, 77, 87, 94, 99, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\nIt looks like 2 Pac’s song “Baby Don’t Cry” was only on the Billboard Hot 100 for 7 weeks and then dropped off the charts.\n\n\n\nsummary(billboard_2000$rank)\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00   26.00   51.00   51.05   76.00  100.00   18785 \n\n\n\nWe don’t want to keep the 18785 rows with missing ranks."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#pivoting-better-values_drop_na",
    "href": "workshops/data_wrangling_r.html#pivoting-better-values_drop_na",
    "title": "",
    "section": "Pivoting Better: values_drop_na\n",
    "text": "Pivoting Better: values_drop_na\n\nAdding the argument values_drop_na = TRUE to pivot_longer() will remove rows with missing ranks. Since these NAs don’t really represent unknown observations (i.e. they were forced to exist by the structure of the dataset) this is an appropriate approach here.\n\nbillboard_2000 &lt;- billboard_2000_raw %&gt;%\n  pivot_longer(cols = wk1:wk76, \n               names_to = \"week\", \n               values_to = \"rank\", \n               values_drop_na = TRUE)\nsummary(billboard_2000$rank)\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   26.00   51.00   51.05   76.00  100.00 \n\n\n\nNo more NA values!\n\ndim(billboard_2000)\n\n\n\n[1] 5307    7\n\n\n\n\nAnd way fewer rows!"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#parse_number",
    "href": "workshops/data_wrangling_r.html#parse_number",
    "title": "",
    "section": "parse_number()",
    "text": "parse_number()\nThe week column is of the type character, but it should be numeric.\n\nhead(billboard_2000$week)\n\n\n\n[1] \"wk1\" \"wk2\" \"wk3\" \"wk4\" \"wk5\" \"wk6\"\n\n\n\nparse_number() grabs just the numeric information from a character string:\n\nbillboard_2000 &lt;- billboard_2000 |&gt; \n2    mutate(week = parse_number(week))\nsummary(billboard_2000$week)\n\n\n2\n\nYou can use mutate() to overwrite existing columns.\n\n\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    5.00   10.00   11.47   16.00   65.00"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#use-pivot_longer-arguments",
    "href": "workshops/data_wrangling_r.html#use-pivot_longer-arguments",
    "title": "",
    "section": "Use pivot_longer arguments",
    "text": "Use pivot_longer arguments\nAlternatively (and more efficiently), there are a number of optional arguments for pivot_longer that are meant to help deal with naming issues.\n\n\nbillboard_2000 &lt;- billboard_2000_raw %&gt;%\n  pivot_longer(starts_with(\"wk\"), \n               names_to        = \"week\", \n               values_to       = \"rank\",\n               values_drop_na  = TRUE,\n3               names_prefix    = \"wk\",\n4               names_transform = list(week = as.integer))\n\nhead(billboard_2000, 5)\n\n\n3\n\nnames_prefix is used to remove “wk” from the values of week\n\n4\n\nnames_transform converts week into an integer number.\n\n\n\n\n\n\n# A tibble: 5 × 7\n   year artist track                   time   date.entered  week  rank\n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;                   &lt;time&gt; &lt;date&gt;       &lt;int&gt; &lt;int&gt;\n1  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26       1    87\n2  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26       2    82\n3  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26       3    72\n4  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26       4    77\n5  2000 2 Pac  Baby Don't Cry (Keep... 04:22  2000-02-26       5    87"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#multiple-variables-in-column-names",
    "href": "workshops/data_wrangling_r.html#multiple-variables-in-column-names",
    "title": "",
    "section": "Multiple Variables in Column Names",
    "text": "Multiple Variables in Column Names\nA more challenging situation occurs when you have multiple pieces of information crammed into the column names, and you would like to store these in separate new variables.\n\nThis dataset contains tuberculosis diagnoses collected by the World Health Organization.\n\nwho2\n\n\n\n# A tibble: 7,240 × 58\n   country      year sp_m_014 sp_m_1524 sp_m_2534 sp_m_3544 sp_m_4554 sp_m_5564\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan  1980       NA        NA        NA        NA        NA        NA\n 2 Afghanistan  1981       NA        NA        NA        NA        NA        NA\n 3 Afghanistan  1982       NA        NA        NA        NA        NA        NA\n 4 Afghanistan  1983       NA        NA        NA        NA        NA        NA\n 5 Afghanistan  1984       NA        NA        NA        NA        NA        NA\n 6 Afghanistan  1985       NA        NA        NA        NA        NA        NA\n 7 Afghanistan  1986       NA        NA        NA        NA        NA        NA\n 8 Afghanistan  1987       NA        NA        NA        NA        NA        NA\n 9 Afghanistan  1988       NA        NA        NA        NA        NA        NA\n10 Afghanistan  1989       NA        NA        NA        NA        NA        NA\n# ℹ 7,230 more rows\n# ℹ 50 more variables: sp_m_65 &lt;dbl&gt;, sp_f_014 &lt;dbl&gt;, sp_f_1524 &lt;dbl&gt;,\n#   sp_f_2534 &lt;dbl&gt;, sp_f_3544 &lt;dbl&gt;, sp_f_4554 &lt;dbl&gt;, sp_f_5564 &lt;dbl&gt;,\n#   sp_f_65 &lt;dbl&gt;, sn_m_014 &lt;dbl&gt;, sn_m_1524 &lt;dbl&gt;, sn_m_2534 &lt;dbl&gt;,\n#   sn_m_3544 &lt;dbl&gt;, sn_m_4554 &lt;dbl&gt;, sn_m_5564 &lt;dbl&gt;, sn_m_65 &lt;dbl&gt;,\n#   sn_f_014 &lt;dbl&gt;, sn_f_1524 &lt;dbl&gt;, sn_f_2534 &lt;dbl&gt;, sn_f_3544 &lt;dbl&gt;,\n#   sn_f_4554 &lt;dbl&gt;, sn_f_5564 &lt;dbl&gt;, sn_f_65 &lt;dbl&gt;, ep_m_014 &lt;dbl&gt;, …\n\n\n\n\nThe first two columns are self explanatory but what’s going on with the rest?"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#multiple-variables-in-column-names-1",
    "href": "workshops/data_wrangling_r.html#multiple-variables-in-column-names-1",
    "title": "",
    "section": "Multiple Variables in Column Names",
    "text": "Multiple Variables in Column Names\nData documentation and some minor investigation would lead you to figure out that the three elements in each of these column names are actually data!\n\nThe first piece, sp/sn/rel/ep, describes the method used for the diagnosis\nThe second piece, m/f is the gender (coded as a binary variable in this dataset)\nThe third piece, 014/1524/2534/3544/4554/5564/65 is the age range (014 represents 0-14, for example)\n\n\nTo organize the six pieces of information in this dataset into six separate columns, we use pivot_longer() with a vector of column names for names_to and instructors for splitting the original variable names into pieces for names_sep as well as a column name for values_to!"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#multiple-variables-in-column-names-2",
    "href": "workshops/data_wrangling_r.html#multiple-variables-in-column-names-2",
    "title": "",
    "section": "Multiple Variables in Column Names",
    "text": "Multiple Variables in Column Names\n\nwho2 |&gt; \n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"), \n5    names_sep = \"_\",\n    values_to = \"count\"\n  )\n\n\n5\n\nYou can use names_pattern instead of names_sep to extract variables from more complicated naming scenarios if you are familiar with regular expressions.\n\n\n\n\n\n\n# A tibble: 405,440 × 6\n   country      year diagnosis gender age   count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan  1980 sp        m      014      NA\n 2 Afghanistan  1980 sp        m      1524     NA\n 3 Afghanistan  1980 sp        m      2534     NA\n 4 Afghanistan  1980 sp        m      3544     NA\n 5 Afghanistan  1980 sp        m      4554     NA\n 6 Afghanistan  1980 sp        m      5564     NA\n 7 Afghanistan  1980 sp        m      65       NA\n 8 Afghanistan  1980 sp        f      014      NA\n 9 Afghanistan  1980 sp        f      1524     NA\n10 Afghanistan  1980 sp        f      2534     NA\n# ℹ 405,430 more rows"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#variable-values-in-column-names",
    "href": "workshops/data_wrangling_r.html#variable-values-in-column-names",
    "title": "",
    "section": "Variable & Values in Column Names",
    "text": "Variable & Values in Column Names\nThis dataset contains data about five families, with the names and dates of birth of up to two children.\n\nhousehold\n\n\n\n# A tibble: 5 × 5\n  family dob_child1 dob_child2 name_child1 name_child2\n   &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;      \n1      1 1998-11-26 2000-01-29 Susan       Jose       \n2      2 1996-06-22 NA         Mark        &lt;NA&gt;       \n3      3 2002-07-11 2004-04-05 Sam         Seth       \n4      4 2004-10-10 2009-08-27 Craig       Khai       \n5      5 2000-12-05 2005-02-28 Parker      Gracie     \n\n\n\nThe new challenge in this dataset is that the column names contain the names of two variables (dob, name) and the values of another (child, with values 1 or 2)."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#variable-values-in-column-names-1",
    "href": "workshops/data_wrangling_r.html#variable-values-in-column-names-1",
    "title": "",
    "section": "Variable & Values in Column Names",
    "text": "Variable & Values in Column Names\n\n\nhousehold |&gt; \n  pivot_longer(\n    cols = !family, \n6    names_to = c(\".value\", \"child\"),\n    names_sep = \"_\", \n7    values_drop_na = TRUE\n  )\n\n\n\n6\n\n.value isn’t the name of a variable but a unique value that tells pivot_longer to use the first component of the pivoted column name as a variable name in the output.\n\n7\n\nUsing values_drop_na = TRUE again since not every family has 2 children.\n\n\n\n\n# A tibble: 9 × 4\n  family child  dob        name  \n   &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt; \n1      1 child1 1998-11-26 Susan \n2      1 child2 2000-01-29 Jose  \n3      2 child1 1996-06-22 Mark  \n4      3 child1 2002-07-11 Sam   \n5      3 child2 2004-04-05 Seth  \n6      4 child1 2004-10-10 Craig \n7      4 child2 2009-08-27 Khai  \n8      5 child1 2000-12-05 Parker\n9      5 child2 2005-02-28 Gracie"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#pivot_wider",
    "href": "workshops/data_wrangling_r.html#pivot_wider",
    "title": "",
    "section": "pivot_wider",
    "text": "pivot_wider\npivot_wider() is the opposite of pivot_longer(), which you use if you have data for the same observation taking up multiple rows.\n\nHere’s an example of data that we probably want to pivot wider (unless we want to plot each statistic in its own facet):\n\n\n# A tibble: 6 × 3\n  Group Statistic Value\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 A     Mean       1.28\n2 A     Median     1   \n3 A     SD         0.72\n4 B     Mean       2.81\n5 B     Median     2   \n6 B     SD         1.33\n\n\n\n\nA common cue to use pivot_wider() is having measurements of different quantities in the same column."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#pivot_wider-example",
    "href": "workshops/data_wrangling_r.html#pivot_wider-example",
    "title": "",
    "section": "\npivot_wider Example",
    "text": "pivot_wider Example\n\nwide_stats &lt;- long_stats |&gt; \n8  pivot_wider(id_cols = Group,\n9              names_from = Statistic,\n10              values_from = Value)\nwide_stats\n\n\n8\n\nid_cols is the column that uniquely identifies each row in the new dataset. Default is everything not in names_from and values_from.\n\n9\n\nnames_from provides the names that will be used for the new columns\n\n10\n\nvalues_from provides the values that will be used to populate the cells of the new columns.\n\n\n\n\n\n\n# A tibble: 2 × 4\n  Group  Mean Median    SD\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 A      1.28      1  0.72\n2 B      2.81      2  1.33\n\n\n\npivot_wider() also has a number of optional names_* and values_* arguments for more complicated transformations.\n\n\n\n\n\n Nested Data\n\n\nIf there are multiple rows in the input that correspond to one cell in the output you’ll get a list-column. This means that you 1) need to fix something in your code/data because it shouldn’t be nested in this way or 2) need to use unnest_wider() or unnest_longer() in order to access this column of data. More on this here."
  },
  {
    "objectID": "workshops/data_wrangling_r.html#useful-resources",
    "href": "workshops/data_wrangling_r.html#useful-resources",
    "title": "",
    "section": "Useful Resources",
    "text": "Useful Resources\n\nCheatsheets:\n\nreadr\ndplyr\ntidyr\n\nIntroductory Book:\nR for Data Science (2e) by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund"
  },
  {
    "objectID": "workshops/data_wrangling_r.html#csscr-consulting-services",
    "href": "workshops/data_wrangling_r.html#csscr-consulting-services",
    "title": "",
    "section": "CSSCR Consulting Services",
    "text": "CSSCR Consulting Services\nCSSCR is a resource center for the social science departments1 at the University of Washington.\n\nAs you continue to learn R feel free to drop by with any/all of your R coding questions. Below are our hours for the quarter:\n\n\n\n\n\n\n\n\n\n\n\n\nThe Center for Social Science Computation and Research\n\n\nConsulting Hours for Fall 2025\n\n\n\nIn-Person1\n\nVirtual2\n\n\n\n\n\nMonday\n8am - 6pm\n6 - 9pm\n\n\nTuesday\n8am - 6pm\n6 - 9pm\n\n\nWednesday\n8am - 6pm\n6 - 9pm\n\n\nThursday\n8am - 6pm\n6 - 9pm\n\n\nFriday\n8am - 5pm\n\n\n\n\n\n1 Drop-in @ Savery 119 2 Online Via Zoom\n\n\n\n\n\n\n\nConstituent member departments include The College of Education, The Department of Anthropology, The Department of Communication, The Department of Economics, The Department of Geography, The Department of Political Science, The Department of Psychology, The Department of Sociology, The Jackson School of International Studies, and The School of Social Work"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "I’m still working on this page! Check back soon for updates and in the meantime click the Google icon to the right to see my published work."
  },
  {
    "objectID": "workshops/text_analysis.html",
    "href": "workshops/text_analysis.html",
    "title": "Source Material",
    "section": "",
    "text": "Introduction to  Text Analysis in R\nCSSCR Workshop\n30 January 2025\nVictoria Sass"
  },
  {
    "objectID": "workshops/text_analysis.html#footnotes",
    "href": "workshops/text_analysis.html#footnotes",
    "title": "Source Material",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can read the official manifesto here.↩︎\nIllustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst↩︎\nPlacing variables in columns also leverages R’s vectorized nature, i.e. most built-in R functions work with values of vectors.↩︎\nIn fact, all tidyverse functions are designed to work with tidy data.↩︎\nIllustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst↩︎\nAs a dataset for topic modeling, that is. Not as the perfect comfort show!↩︎"
  },
  {
    "objectID": "workshops/intro_r_rstudio.html",
    "href": "workshops/intro_r_rstudio.html",
    "title": "",
    "section": "",
    "text": "Introduction to R & RStudio\nIntroduction to R & RStudio\nCSSCR Workshop\nCSSCR Workshop\nVictoria Sass"
  }
]